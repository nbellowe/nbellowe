---
layout: post
title: Temporary notes page I just need this online
tags: ["code", "drafts", "todo"]
---
title: Temporary notes page I just need this online

### Introduction to Operating Systems

# What is an Operating System?
### Name some OSs:
 -  Windows, Linux, Mac OS X, Google Android, …
 -  >500 at http://www.operating-system.org/betriebssystem/\_english/os-liste.htm
 -  >600 versions of Linux!  i.e. over 600 distributions of Linux.
### What is common across these OSs?

# OS Historical Timeline
 -  1930s/40s – electronic digital computers arise (ENIAC 1946 is 1st general-purpose programmable digital computer)
 -  1950s – 1st OSs begin to emerge
 -  1961 – MIT's CTSS is the first time-sharing system
 -  1966 – IBM System/360 mainframe OS
 -  1969 - UNIX for mainframes and minicomputers
 -  1981 – MS-DOS OS for personal computers
 -  1982 – 4.2 BSD Unix with TCP/IP networking
 -  1984 – Mac OS with windowing GUI
 -  1991 – Linux open source OS for PC
 -  Late 1990s/early 2000s – virtual machines arise like VMWare, Xen, …
 -  2007 – iOS for iPhone
### Name some types of applications
 -  Web browers, video players, games, Office apps, …
### An operating system is a layer of software between many applications and diverse hardware that
### Provides a hardware abstraction so an application doesn't have to know the details about the hardware.
 -  So an application saving a file to disk doesn't have to know how the disk operates
### Arbitrates access to resources among multiple applications: Sharing  of resources
### Provides Protection:
Isolation protects app's from each other
Isolation also to protect the OS from applications
Isolation to limit resource consumption by any one app

# Operating System Components
 -  A typical operating system consists of multiple components
 -  A process manager with a scheduler, thread management, and atomic synchronization

# Operating System Hierarchy
 -  OS components may be built on top of other OS components
 -  The file system is usually built on top of the device manager
 -  File system supports high level abstraction/concept of files
 -  Device manager handles low level interaction with devices
# Are System Libraries/Tools part of an OS?

# What's in an OS?  Monolithic vs Microkernel OS Architectures
 -  Linux has a monolithic kernel
 -  The kernel is highly complex and contains many components
 -  Mach OS has a microkernel
 -  The kernel has minimal functionality, perhaps only virtual memory, scheduler, and IPC message passing
 -  All other components are viewed as outside the OS, and communicate via message passing.
 -  Advantage: bug in a component doesn't crash whole kernel, arguably easier to manage
 -  Disadvantage: message passing was slow
 -  Mac OSX is a hybrid of Mach and BSD Unix

# Distributed Operating Systems
 -  Example 1: Distributed File System
 -  OS adds TCP/IP Network Stack
 -  Device driver support for networking cards
 -  Files can now be written/read remotely

# Outline of the OS course
 -  Hardware support, user/supervisor mode, system calls, trap table, device I/O, interrupts, DMA, mem-mapped I/O
 -  Processes, threads, scheduling, synchronization, deadlock
 -  Memory management, paging, virtual memory
 -  File system design, allocation, networked file systems
 -  Security: authorization, access control
 -  Networked OSs: distributed file systems
 -  Virtual machines

# Operating System Trends
 -  Hardware has evolved quickly - OS must adjust
 -  Moore's Law roughly applies to CPU speed and/or memory size: doubles every 18 months => exponential!
 -  Enables complex modern operating systems like Linux, Windows, UNIX, OS X

# Operating System Trends
 -  Diversification of OS's to many different target environments
 -  Energy-efficient cell phone OSs - scaling down
 -  iPhone's iOS, Google's Android, …
 -  Multi-processor OSs - scaling up
 -  Adapting Linux and Windows to multiple cores.  Massively parallel supercomputers.
 -  Real-Time OS for Embedded and Multimedia Systems
 -  VXWorks, robotic OSs, …

# Operating System Trends
 -  Virtualization – Virtual Machines (VMs)
 -  Running a Windows VM inside a Linux OS, and vice versa.
 -  More layers of abstraction
 -  Cloud computing rents VMs on racks of PCs at a massive scale
# Cloud Computing
 -  Cloud computing (continued)
# Elastic OS – Prof. Han's research project
 -  Expand/contract Linux over multiple rack machines

# Chapters 1 and 2: 
# Kernel Mode, Traps, System Calls, Multitasking

# Protection in Operating Systems
 -  One of an Operating System's main goals is Protection
 -  Protect applications from each other
 -  Protect OS from applications

# Memory Protection via Virtual Memory
 -  Recall that an executable only has virtual addresses
 -  These are translated into physical memory addresses at run time by a page table
 -  OS controls the page table
 -  Difficult for a program to write into another program or kernel's address space
 -  Any virtual address given to memory manager is translated into a non-conflicting physical address
 -  Access to the "wrong" memory causes a page fault
 -  Caveats: shared libraries, …

# Protecting the OS via a Mode Bit
 -  Processors include a hardware mode bit that identifies whether the system is in user mode or supervisor/kernel mode
 -  Requires extra support from the CPU hardware for this OS feature
 -  Prevents applications from executing privileged instructions
 -  Can't reset time slice register, or change interrupt vector register, …
 -  Embedded microcontrollers don't have mode bit
 -  80286 added mode bit in 1982

# Kernel Mode vs User Mode
 -  Supervisor or kernel mode (mode bit = 0)
 -  Can execute all machine instructions, including privileged instructions
 -  Can reference all memory locations
 -  Kernel executes in this mode
 -  User mode (mode bit = 1)
 -  Can only execute a subset of non-privileged instructions
 -  Can only reference a subset of memory locations
 -  All applications run in user mode

# Multiple Rings/Modes of Privilege
 -  Intel x86 CPUs support four modes or rings of privilege
 -  Common configuration:
 -  OS like Linux or Windows runs in ring 0 (highest privilege), Apps run in ring 3, and rings 1-2 are unused
## Microkernel System Structure
 -  System Calls: How Apps and the OS Communicate
 -  The trap instruction is used to switch from user to supervisor mode, thereby entering the OS trap sets the mode bit to 0
 -  On x86, use INT assembly instruction (more recently SYSCALL/SYSENTER) mode bit set back to 1 on return
 -  Any instruction that invokes trap is called a system call
 -  There are many different classes of system calls
## API – System Call – OS Relationship
# Trap Table
 -  The process of indexing into the trap table to jump to the trap handler routine is also called dispatching
 -  The trap table is also called a jump table or a branch table "A trap is a software interrupt"
 -  Trap handler (or system call handler) performs the specific processing desired by the system call/trap

## System Call Parameter Passing
Often, more information is required than simply identity of desired system call
Exact type and amount of information vary according to OS and call
Three general methods used to pass parameters to the OS
Simplest:  pass the parameters in _registers_
 In some cases, may be more parameters than registers
_Pointer_: Parameters stored in a block_,_ or table, in memory, and address of block passed as a parameter in a register
This approach taken by Linux and Solaris
Parameters placed, or **pushed** _,_ onto the **stack** by the program and **popped** off the _stack_ by the operating system
Block and stack methods do not limit the number or length of parameters being passed
## Parameter Passing via Table

# Classes of System Calls Invoked by trap
end, abort
load, execute
fork, create, terminate
get attributes, set
wait for time
wait event, signal event
allocate memory, free
## Standard C Library Example
C program invoking printf() library call, which calls write() system call

# How does an OS support multiple applications?  Batching of jobs
# What happens if some programs are I/O-bound?
# Limitations of Sequential Execution
 -  Program P1 blocks waiting for something to complete waiting on I/O, e.g. waiting for a disk write to complete, or waiting to read data from a keyboard I/O can be very slow compared to CPU speed then CPU is idle for potentially billions of cycles!
 -  Better if CPU switches to another program P2 and begins executing P2 better utilization of the CPU for I/O-bound programs, e.g. shells, editors (talk to keyboard and disk)

# Multiprogramming
# Multiprogrammed Batch Systems
# Limitations of Multiprogramming
 -  Batch jobs are very non-interactive
 -  Don't support a shell application for example design jobs to yield much sooner than an I/O block, to give the impression of interactivity
 -  If one of the programs was a shell, then it appears to the human user as if the computer is instantly responsive
 -  In the small time segment a shell is given, it can draw a character on the screen that you've just typed => appearance of real-time interactivity

# Multitasking
 -  CPU rapidly switches between multiple programs
 -  Each program gets a small slice of the CPU, then yields the CPU to another program
 -  This switching happens often enough that each program still gets a fair percentage of the CPU, and can still make significant progress
 -  At the same time, interactive programs like shells are now supported – this was a big innovation

# Context Switch Overhead
 -  Switching from one program to another is called a context switch there is overhead due to this context switching With each context switch, the CPU has to save the current state of application 1 (its PC, IR, data registers, stack pointer, etc.), and then load the state of the new application 2 when app 2 was last switched out (new PC, new IR, new data registers, new stack pointer, etc.) All of this takes time - typical overhead = 1s,
 -  No useful work can be done by program during a context switch

# Cooperative Multitasking
 -  How does an OS achieve Multitasking?
 -  Cooperative multitasking
 -  Preemptive multitasking
 -  In cooperative multitasking, programs quickly and voluntarily yield CPU before they're done
 -  Like batch mode multiprogramming, except it's more fine-grained than jobs and yielding is more explicit than opportunistic
 -  Early OSs did this (Windows 3.1, Mac OS 9.\*)

# Preemptive Multitasking Time Slices
 -  Each program is given a short interval on the CPU called a time slice
 -  Typical time slice is 30 ms
 -  The length of the default time slice is a compile time option for the OS kernel
 -  Also some schedulers vary the time slice according to the priority of the process, e.g. higher priority processes get a longer time slice.

# Preemptive Multitasking Interrupts
 -  Timer interrupt fires periodically
 -  This suspends execution of the currently executing program and returns control to the OS scheduler
 -  The scheduler decides the next program to execute and loads it, then passes control to it
 -  Switching from one program to another is called a context switch there is overhead due to this context switching
 -  Overhead is only 1s per time slice of 30 ms, so overhead % = 1/30000 = .003 %

# Preemptive Multitasking Benefits
 -  Efficient sharing of CPU
 -  Programs blocked on I/O don't block other programs
 -  Fault isolation
 -  Programs are forced to yield, so can't block other programs
 -  Support for long-running jobs
 -  Support for interactive programs

# Preemptive Multitasking History
 -  Early computers were big mainframes
 -  But wanted to share the CPU of a mainframe not just between different batch jobs, but also between different human users
 -  Interactive time sharing systems were developed
 -  Time-sharing examples
 -  multiple processes sharing time locally on a CPU
 -  multiple user terminals remotely sharing processing time with a central server keystroke delay during heavy loads (before a class assignment was due) could be significant and non-interactive
 -  Basically all modern operating systems are preemptively multitasked
 -  Linux, BSD Unix, Windows NT/XP/Vista, Mac OS X 10.\*,

# Time-Sharing Computers
# Multitasking & Abstract Machines
 -  CPU is time-multiplexed between multiple programs Programs share the CPU
 -  Memory is space-multiplexed between multiple programs programs share RAM
 -  Each program thus sees its own abstract machine (provided by OS)
 -  it has its own "private" (slower) CPU,  it has its own "private" (smaller) memory

 -  Examples of Windows and  Unix System Calls
## Types of System Calls
Process control
create process, terminate process
end, abort
load, execute
get process attributes, set process attributes
wait for time
wait event, signal event
allocate and free memory
Dump memory if error
**Debugger** for determining **bugs, single step** execution
**Locks** for managing access to shared data between processes

## Types of System Calls
File management
create file, delete file
open, close file
read, write, reposition
get and set file attributes
Device management
request device, release device
read, write, reposition
get device attributes, set device attributes
logically attach or detach devices

## Types of System Calls (Cont.)
Information maintenance
get time or date, set time or date
get system data, set system data
get and set process, file, or device attributes
Communications
create, delete communication connection
send, receive messages if **message passing model** to **host name** or **process name**
From **client** to **server**
**Shared-memory model** create and gain access to memory regions
transfer status information
attach and detach remote devices

## Types of System Calls (Cont.)
Protection
Control access to resources
Get and set permissions
Allow and deny user access

# Port-Mapped I/O
# Port-Mapped I/O Implications
 -  I/O address space is different/separate from main memory's address space
 -  Need special hardware instructions to support port-mapped I/O
 -  Only OS (device driver) in kernel mode can execute these instructions
 -  Device I/O Port Locations on PCs (partial)

# Port-Mapped I/O Limitations
 -  IN and OUT can only store and load
 -  Don't have full range of memory operations for normal CPU instructions
 -  Example: to increment the value in say a device's data register, have to copy register value into memory, add one, and copy it back to device register.
 -  With memory-mapped I/O, can increment value in memory directly, and it gets reflected into the device controller's data register automatically
 -  Difficult for application developers
 -  Application has to save data to be written, then call write()
 -  Typically transfer very small amounts of data

 -  Map a device's registers and memory to portions of main memory
 -  Reserve a block of memory M1 to map to the device D1's registers and memory, memory block M2 maps to D2, …
 -  To read from/write to a device, just reference the memory location, and the OS+hardware will fulfill the request automatically
 -  For example, writing to a mapped memory location will automatically write the data into the I/O device
 -  e.g. move R3, 0x00FF01 the memory address 0x00FF01 is mapped to the I/O device's registers

 -  Memory Management Unit (MMU) maps memory values and data to/from device registers
 -  Device registers are assigned to a block of memory
 -  When a value is written into that I/O-mapped memory, the device sees the value, loads the appropriate value and executes the appropriate command
 -  i.e. I/O devices are listening on the shared address lines, and when they see their memory-mapped address, and say a write, they will intercept that and write to their device's memory or registers

 -  How does a device learn its memory-mapped address?
 -  OS will tell the device its assigned memory-mapped address range by writing into a special set of device registers called the base address registers
 -  These addresses will be non-conflicting with other addresses mapped to other devices
 -  Auto-configuration is a complicated topic

# Memory-Mapped I/O (4)
 -  Typically, devices are mapped into lower memory in x86
 -  frame buffers for displays take the most memory, since most other devices have smaller buffers
 -  Even a large display might take only tens of MB of memory
 -  memory-mapped I/O is a small penalty, since tens of MB of space is small vs many GBs of memory available in RAM

# Memory-Mapped I/O Advantages
 -  Example: rendering data into the graphics card's frame buffer.  Old approach:
 -  A program firsts renders image to be displayed into main memory
 -  Then the program calls high-level API to write image into frame buffer
 -  This invokes OS to write each byte to device (read a byte into CPU register and call OUT for each byte to the device).
 -  Advantage: Faster data transfer
 -  With memory-mapped I/O, step 3 is eliminated because writes to addresses in step 1 are memory-mapped and are immediately written into the frame buffer at the correct location
 -  Programming simplified
 -  Step #2 is eliminated (previous slide), because writes to memory-mapped addresses corresponding to the frame buffer are immediately written into the frame buffer
 -  No extra hardware instructions are needed
 -  Normal move instructions suffice
 -  I/O and memory share the same address space, i.e. same control, data and address buses
 -  For these reasons, memory-mapped I/O is more popular than port-mapped I/O

# Adding Device Drivers
 -  Unsatisfactory approach: device drivers are statically linked into the OS kernel
 -  Requires kernel to be recompiled each time a new device was added
 -  Want OS to support new devices (which have their own device drivers) without recompiling the kernel each time
 -  Solution: kernel modules
 -  Essentially these are dynamically linked software objects
 -  Most modern OSs support this way to add functionality to the kernel

# Bootstrapping the OS in PCs
 -  Multi-stage procedure:
 -  Power On Self Test (POST) from ROM
 -  Check hardware, e.g. CPU and memory, to make sure it's OK
 -  BIOS (Basic Input/Output System) looks for a device to boot from…
 -  May be prioritized to look for a USB flash drive or a CD/DVD-ROM drive before a hard disk drive
 -  Can also boot from network
 -  BIOS finds a hard disk drive to boot from
 -  Looks at Master Boot Record (MBR) in sector 0 of disk
 -  Only 512 bytes long (Intel systems), contains primitive code for later stage loading and a partition table listing an active partition, or the location of the bootloader

 -  Multi-stage procedure: (continued)
 -  Primitive loader then loads the secondary stage bootloader
 -  Examples of this bootloader include LILO (Linux Loader), and GRUB (Grand Unified Bootloader)
 -  Can select among multiple OS's (on different partitions) – i.e. dual booting
 -  Once OS is selected, the bootloader goes to that OS's partition, finds the boot sector, and starts loading the OS's kernel
# Loading a Program into Memory
# Loading and Executing a Program
# Loading and Executing a Program
# Loading Executable Object Files
 -  When a program is loaded into RAM, it becomes an actively executing application
 -  The OS allocates a stack and heap to the app in addition to code and global data.
 -  A call stack is for local variables
 -  A heap is for dynamic variables, e.g. malloc()
 -  Usually, stack grows downward from high memory, heap grows upward from low memory
# Running Executable Object Files
Stack contains local variables
As main() calls function  f1, we allocate f1's local variables on the stack
If f1 calls f2, we allocate f2's variables on the stack below f1's, thereby growing the stack, etc…
When f2 is done, we deallocate f2's local variables, popping them off the stack, and return to f1
Stack dynamically expands and contracts as program runs and different levels of nested functions are called
Heap contains run-time variables/buffers
Obtained from malloc()
Program should free() the malloc'ed memory
Heap can also expand and contract during program execution
# Multiple Applications + OS

# Chapter 3: What is a Process?
 -  A software program consist of a sequence of code instructions and data stored on disk
 -  A program is a passive entity
 -  A process is a program actively executing from main memory within its own address space
 -  A process is a program actively executing from main memory has a Program Counter (PC) and execution state associated with it
CPU registers keep state
OS keeps process state in memory alive!
 -  Owns its own address space limited set of (virtual) addresses that can be accessed by the executing code
 -  2 processes may execute the same program code, but they are considered separate execution sequences e.g. two shell terminals

# A Process Executes in its Own Address Space
 -  OS tries to provide the illusion or abstraction to the process that it executes on its own abstract machine in its own subset of RAM, i.e. its own address space – achieved using virtual memory paging on its own subset (time slice) of the CPU – achieved by preemptive multitasking

# How is a Process Structured in Memory?
 -  Run-time memory image
 -  Essentially code, data, stack, and heap
 -  Code and data loaded from executable file
 -  Stack grows downward, heap grows upward
# Applications and Processes
 -  a server could be split into multiple processes, each one dedicated to a specific task (UI, computation, communication, etc.)
 -  The Application's various processes talk to each other using Inter-Process Communication (IPC).  We'll see various forms of IPC later.

# Chapters 3 & 4: Process Management, Threads
 -  Multi-stage Bootloading of OS
 -  POST, BIOS, GRUB, then OS

# Ch 13: ioctl and fcntl (input/output control)
 -  Want a richer interface for managing I/O devices then just open, close, read, write
 -  ioctl allows a user-space application to configure parameters and/or actions of a hardware I/O device
 -  e.g set the speed of a device, or eject a disk
 -  Usage:
 -  int ioctl(int fd, int cmd, ...) ;
 -  Invokes a system call to execute device-specific cmd on I/O device fd
 -  Used for I/O operations and other operations which cannot be expressed by regular system calls
 -  Requests are directed to the correct device driver
 -  Avoids having to create new system calls for each new  device and/or unforeseen device function
 -  Helps make the OS/kernel extensible
 -  UNIX, Linux, MacOS X all support ioctl, and Windows has its own version
 -  In UNIX, each device is modeled as a file
 -  fcntl
 -  for file control is related to ioctl and is used for configuring file parameters, hence in many cases I/O communication
 -  e.g. use fcntl to set a network socket to non-blocking
 -  Part of POSIX API, so portable across platforms

# Process Management
end, abort load, execute fork, create, terminate get attributes, set wait for time wait event, signal event allocate memory, free

# Process Manager
 -  Creation/deletion of processes (and threads)
 -  Synchronization of processes (and threads)
 -  Managing process state
 -  Processor state like PC, stack ptr, heap ptr, etc.
 -  Resources like open files, sockets, etc.
 -  Memory limits to enforce an address space
 -  Scheduling processes
 -  Monitoring processes

# Process Control Block (PCB)
Process state, e.g. ready, running, or waiting
accounting info, e.g. process ID
Program Counter
CPU registers
CPU-scheduling info, e.g. priority
Memory management info, e.g. base and limit registers, page tables
I/O status info, e.g. list of open files

# Creating Processes
 -  In Windows, there is a CreateProcess() call
 -  Pass an argument to CreateProcess() indicating which program to start running
 -  Invokes a system call to OS that then invokes process manager to:
 -  allocate space in memory for the process
 -  Set up PCB state for process, assigns PID, etc.
 -  Copy code of program name from hard disk to main memory, sets PC to entry point in main
 -  Schedule the process for execution
 -  As we will see, this combines UNIX's fork() and exec() and achieves the same effect

# Creating Processes in UNIX
 -  Use fork() command to create/spawn new processes from within a process
 -  When a parent process calls fork(), this creates a child process that inherits a copy of the parent's code;
 -  In UNIX, the child starts executing at the same point as the parent, namely just after returning from the fork() call
 -  Typically, only the child then calls exec() to copy in code from the new program to run

# Forking Processes
PID = fork();
   if (PID==0) { /\* child \*/
      codeforthechild();
      exit(0);
   }
   /\* parent's code here \*/

 -  In UNIX, the fork() call returns
 -  The child's PID in the parent
 -  Zero in the child
# Fork() conceptually...
 -  Fork() duplicates address space of parent in the child
 -  Both execute concurrently
# Loading New Processes' Code
   PID = fork();
   if (PID==0) { /\* child \*/
      exec("/bin/ls");
      exit(0);
   }
   /\* the parent's code here \*/

 -  the exec() system call loads program code into the calling process's memory (same address space!) - the calling code is erased!  -  clears the stack, and begins executing the new code at its main entry point

# Copy-On-Write
 -  copying the entire code of a parent into a child can be expensive on a fork() or CreateProcess()
 -  Better: let the child share the parent's code pages
 -  Accomplish this by having the page tables point to the same physical pages in memory
 -  Only create a copy of a page on a write.
 -  Since the code is read-only, then never have to create a 2 nd copy of the code pages.
 -  This speeds up creation of new processes
# Deleting Processes
 -  In UNIX, the wait() call is used by a parent process to be informed of when a child has completed, i.e. called exit()
 -  Once the parent has called wait(), the child's PCB and address space can be freed
 -  This is also called reaping
 -  There is also waitpid() to wait on a particular child process to finish

# Process Context Switches
 -  A context switch can occur because of a system call an I/O interrupt, e.g. disk has finished reading a timer interrupt
 -  Context switch time is pure overhead
 -  Have to save the state of the process 1 in the PCB1
 -  Then have to load the state of process 2 from PCB2
 -  Typically on the order of microseconds vs time slices of tens of milliseconds
# Accessing Process State
 -  One way is through standard system calls
 -  Another way is through the /proc "pseudo"-file system interface on Linux systems
 -  Linux exports process status through /proc
 -  Each process is listed by its process ID in the /proc directory
 -  To inspect a given variable of a process, look up its corresponding file name
 -  e.g. /proc/processID/stat gives the process' status
# Using /proc
 -  Can read and write status variables
 -  Most /proc files are read-only
 -  sysctl can be used to change a limited # of kernel variables can tune kernel at run-time
 -  Many system utilities like ps (process status) and top are simply calls to files in the /proc directory
# /proc to Access System State
 -  /proc also contains information about
 -  hardware I/O devices
 -  overall system status
 -  not just process state
 -  Examples:
 -  /proc/interrupts shows which interrupts are in use, and how many of each there have been
 -  /proc/devices lists the device drivers configured into the currently running kernel
 -  /proc/net contains networking information

# Threads
 -  A thread is a logical flow or unit of execution that runs within the context of a process has its own program counter (PC), register state, and stack
 -  shares the memory address space with other threads in the same process,
 -  share the same code and data and resources (e.g. open files)
 -  A thread is also called a lightweight process

# Threads
Process P1 is _multithreaded_
Process P2 is single threaded
The OS is _multiprogrammed_
If there is preemptive timeslicing, the system is _multitasked_

# Motivation for Threads
 -  reduced context switch overhead vs multiple processes
 -  In Solaris, context switching between processes is 5x slower than switching between threads
 -  Don't have to save/restore context, including base and limit registers and other MMU registers, also TLB cache doesn't have to be flushed
 -  shared resources => less memory consumption
 -  Don't duplicate code, data or heap or have multiple PCBs as for multiple processes
 -  Supports more threads – more scalable, e.g. Web server must handle thousands of connections
# Motivation for Threads (2)
 -  inter-thread communication is easier and faster than inter-process communication
 -  threads share the same memory space, so just read/write from/to the same memory location!
 -  IPC via message passing uses system calls to send/receive a message, which is slow
 -  IPC using shared memory may be comparable to inter-thread communication
# Applications, Processes, and Threads
 -  Application =  Processes i
 -  e.g. a server could be split into multiple processes, each one dedicated to a specific task (UI, computation, communication, etc.)
 -  Process =  Threads j
 -  e.g. a Web server process could spawn a thread to handle each new http request for a Web page
 -  An application could thus consist of many processes and threads

# Thread Safety
Suppose Thread 1 wants to increment global variable X Thread 2 wants to decrement variable X
_Could have a race condition (see Chapter 6)_
# Thread-Safe Code
 -  "A piece of code is thread-safe if it functions correctly during simultaneous or concurrent execution by multiple threads."
 -  "In particular, it must satisfy the need for multiple threads to access the same shared data, and the need for a shared piece of data to be accessed by only one thread at any given time."
 -  If two threads share and execute the same code, then unprotected use of shared global variables is not thread safe static variables is not thread safe heap variables is not thread safe
 -  The use of local variables is thread safe To make a thread-unsafe code thread-safe, add locking mechanisms to protect access to shared resources thread-safe code protects and synchronizes access to global and static variables, i.e. persistent data, with locking and synchronization mechanisms
 -  Rewrite code to only use local variables and parameters, and make sure any called functions do the same thing, and so on down the call chain…

 -  function f() {
 -  lock();
 -                  change global variable G;
 -  unlock();
 -          }

# Reentrant Code
 -  Reentrancy is a concept related to but different from thread safety
 -  Code is reentrant if a single thread (really, a sequence of execution) can be interrupted in the middle of executing the code and then can reenter the same code later in a safe manner (before the first entry has been completed)
 -  In contrast, code is thread safe if multiple threads can operate safely in the same code at the same time
# Reentrant Code (2)
 -  Reentrancy was developed to describe interrupt service routines (ISRs) in early OSs, i.e. interrupt handlers in the middle of an OS processing an interrupt, its ISR can be interrupted to process a 2 nd interrupt
 -  The same OS ISR code may be reentered a 2 nd time before the 1 st interrupt has been fully processed
 -  If the ISR code is not well-written, i.e. reentrant, then the system could hang or crash

# Reentrant Code Example
# Thread-safe but not reentrant
 -  Earlier example of code below was thread-safe but not reentrant:
 -  function f() {
 -                  lock();
 -                  change global variable G;
 -                  unlock();
 -          }
 -  Not reentrant because if f() is interrupted just before the unlock(), and f() is called a 2 nd time, the system will hang, because the 2 nd call will try to lock, and be unable to lock, because the 1 st call had not yet unlocked the system

# Processes vs. Threads
 -  Why are processes still used when threads bring so many advantages?
 -  Some tasks are sequential and not easily parallelizable, and hence are single-threaded by nature
 -  No fault isolation between threads
 -  If a thread crashes, it can bring down other threads
 -  If a process crashes, other processes continue to execute, because each process operates within its own address space, and so one crashing has limited effect on another
Caveat: a crashed process may fail to release synchronization locks, open files, etc., thus affecting other processes .  But, the OS can use PCB's information to help cleanly recover from a crash and free up resources.
 -  Why are processes still used when threads bring so many advantages? (cont.)
 -  Writing thread-safe/reentrant code is difficult.  Processes can avoid this by having separate address spaces and separate copies of the data and heap

# User-Space Threads
 -  User space threads
 -  are usually cooperatively multitasked, i.e. user threads within a process voluntarily give up the CPU to each other
 -  provides interface to create, delete threads in the same process
 -  threads will synchronize with each other via the user space threading package or library
 -  OS is unaware of user-space threads – only sees user-space processes
 -  If one user space thread blocks, the entire process blocks in a many-to-one scenario (see text)
 -  pthreads is a POSIX threading API implementations of pthreads API differ underneath the API; could be user space threads; there is also pthreads support for kernel threads as well
 -  User space thread also called a fiber

# Kernel Threads
 -  Kernel threads are supported by the OS kernel sees threads and schedules at the granularity of threads
 -  In user space threads, the kernel doesn't see threads, only processes, and only schedules processes
 -  Most modern OSs like Linux, Mac OS X, Win XP support kernel threads
 -  mapping of user-level threads to kernel threads is usually one-to-one, e.g. Linux and Windows, but could be many-to-one, or many-to-many
 -  Win32 thread library is a kernel-level thread library
# User-Space & Kernel Threads
 -  Java thread library is running in Java VM on top of host OS, so on Windows it's implemented on top of Win32 threading, while on Linux/Unix it's implemented on top of pthreads

# Chapter 3: IPC, Pipes and Signals
# Inter-Process Communication (IPC)
 -  Motivation an application is split into multiple processes that need to share data
 -  How do two processes communicate?
 -  Shared Memory
 -  Message Passing

# Shared Memory IPC
 -  OS creates a shared memory buffer between processes
 -  Advantages:
 -  allows fast read/write by just using pointers
 -  Enables high volume of reading/writing, e.g. moving large files between processes
 -  Common implementation is via virtual memory:
 -  Page tables of both processes point to same pages in memory!
 -  So shared memory is mapped into the address spaces of both processes, rather than being a separate piece
 -  In practice, the shared memory is split into pages and scattered throughout main memory

# Shared Memory Usage
 -  shmid = shmget (key name, size, flags)
 -  Part of the POSIX API that creates a shared memory segment, using a name (key ID)
 -  All processes sharing the memory need to agree on the key name in advance.
 -  Creates a new shared memory segment if no such shared memory with the same name exists and returns handle to the shared memory.
# Shared Memory Usage
 -  shm\_ptr = shmat (shmid, NULL, 0)
 -  to attach a shared memory segment to a process' address space
 -  This association is also called binding Reads and writes now just use shm\_ptr shmctl () modify control information and permissions related to a shared memory segment, & to remove a shared memory segment

# Shared Memory IPC Limitations
 -  Problem: shared access to the same memory introduces potential race conditions
 -  need to synchronize access
 -  Producer-Consumer example
 -  if two producers write at the same time to shared memory, then they can overwrite each other's data
 -  if a producer writes while a consumer is reading, then the consumer may read inconsistent data
 -  Used send () and receive () to communicate messages between processes
 -  Special "ports" are used for communicating messages
 -  Advantage: doesn't require synchronization
 -  Blocking send() allows OS to serialize writes, mitigating race conditions
 -  Disadvantage: Slow
 -  OS is invoked via a system call for each IPC operation to pass control signaling and possibly data as well
 -  Message Passing IPC types:  pipes, UNIX-domain sockets, Internet domain sockets, message queues, and remote procedure calls (RPC)

# Message Passing IPC
 -  Message Passing IPC types:
 -  UNIX-domain sockets
 -  Internet domain sockets
 -  Pipes
 -  Signals
 -  message queues
 -  remote procedure calls (RPC)
# Using Sockets for UNIX IPC
 -  Sockets are an example of message-passing IPC.  Created in UNIX using socket() call:
 -  sd = socket(int domain, int type, int protocol);

# Using Sockets for UNIX IPC (2)
 -  Each communicating process will first create its own socket, usually SOCK\_STREAM.
 -  For UNIX domain sockets (PF\_UNIX domain):
 -  Used only for local communication only among a computer's processes
 -  Emulates reading/writing from/to a file
 -  Each process
 -  bind()
 -  's its socket to a filename:
 -     bind(sd, (struct sockaddr \*)&local, length);

# Using Sockets for UNIX IPC (3)
 -  Usually, one process acts as the server, and the other processes connect to it as clients
# IPC via Internet domain sockets
 -  Similar to Unix domain sockets,
 -  Configure the socket with domain PF\_INET
 -  Set destination to
 -  localhost
 -  (say 127.0.0.1) instead of the usual remote Internet IP address
 -  Choose
 -  a well-known
 -  port # that is shared between processes, i.e. P1 and P2 know this port # in advance
 -  similar to a well-known file name
 -  Both processes then send() and receive() messages via this port and socket
 -  Arguably more portable than UNIX-domain sockets
 -  May be slower than UNIX-domain sockets because messages traverse network's layered stack

# IPC via Pipes
 -  Process 1 writes into one end of the pipe, & process 2 reads from other end of the pipe
 -  e.g. "ls | more"
 -  Form of IPC similar to message-passing but data is viewed as a stream of bytes rather than discrete messages
 -  was one of UNIX's original forms of IPC

# IPC via Pipes
 -  essentially FIFO buffers accessed like file I/O
 -  so standard read()/write() for files can be used
 -  Asynchronous/non-blocking send() and blocking/synchronous receive()
 -  Ordinary pipes are one-way
 -  To create two-way pipes, use two opposite one-way pipes

# Parent-Child Pipe Example
 -  Parent process uses
 -  pipe
 -  () system call to create pipe
# IPC via Pipes (3)
 -  Chapter 3 textbook example is more detailed, e.g. closes the unused write fd for child and closes the unused read fd for parent

# Named Pipes
 -  Traditional one-way or anonymous pipes only exist transiently between the two processes connected by the pipe
 -  As soon as these processes complete, the pipe disappears
 -  Named pipes persist across processes
 -  Operate as FIFO buffers or files, e.g. created using mkfifo(unique\_pipe\_name) on Unix
 -  Different processes can attach to the named pipe to send and receive data
 -  Need to explicitly remove the named pipe
 -  See textbook for more info on named pipes

# Signals as (Limited) IPC
 -  Signals allow a small numerical code to be sent to a process
 -  This interrupts the normal control flow of a process, and is called exceptional control flow
 -  Must register a
 -  handler
 -  to catch this signal using signal() or sigaction()
 -  Usually OS-to-process communication
 -  But some signals are useful in process-to-process communication
 -  e.g. SIGUSR1, SIGCHILD, SIGKILL, etc.
 -  No data can be sent, only the code, so this is very limited IPC
# Signals as (Limited) IPC

# Signals
 -  OS-to-process:
 -  Kernel sets the numerical code in a process variable, then wakes the process up to handle the signal
 -  Process-to-process
 -  Call
 -  kill(process\_id, signal\_num)
 -  e.g.,
 -  kill(Y,SIGUSR1)
 -  sends a SIGUSR1 signal to process Y, which will know how to interpret this signal
 -  Call still goes through OS, not directly from process to process.
 -  A process can send a signal to itself using a library call like alarm()

# Signals
 -  Signals expose low-level hardware exceptions to user processes
 -  May be efficient to get a callback after a read() has completed for example
 -  allows process to keep executing without polling to check for completion
 -  Alternatives:
 -  A process blocks on a read() and is informed of read's completion only when it is unblocked – inefficient!
 -  A process polls a read() – also less efficient
# Linux/UNIX Signals

# Signals
 -  Signals expose low-level hardware exceptions to user processes
 -  May be efficient to get a callback after a read() has completed for example
 -  allows process to keep executing without polling to check for completion
 -  Alternatives:
 -  A process blocks on a read() and is informed of read's completion only when it is unblocked – inefficient!
 -  A process polls a read() – also less efficient

# Signals and Race Conditions
 -  Signals are an asynchronous signaling mechanism in UNIX
 -  A process never knows when a signal will occur
 -  Its execution can be interrupted at any time.
 -  A process must be written to handle this asynchrony.  Otherwise, could get race conditions.

# Signals and Race Conditions
 -  In addition, if there are multiple signals, can have a race conditions
 -  i.e. if a signal handler is processing signal S1 but is interrupted by another signal S2, then could have a race condition inside the handler.
 -  In the previous example, we'd have global++ happening in two handlers in rapid succession, could lead to unpredictable results.
 -  The solution is to block other signals while handling the current signal.
 -  Use sigprocmask () to selectively block other signals
 -  A blocked signal is pending
 -  There can be at most one pending signal per signal type, so signals are not queued

# Signals
 -  Receiving signals:
 -  when a kernel is returning from some exception handler, it checks to see if there are any pending signals for a process before passing control to the process
 -  The user may register a signal handler() via the signal() function.  If no handler is registered, then default action is typically termination
 -  signal(signum, handler) function is used to change the action associated with a signal
 -  if handler is SIG\_IGN, then signals of type signum are ignored
 -  if handler is SIG\_DFL, then revert to default action, usually termination
 -  otherwise if there is a user-defined function handler , then call it to handle the signal.

# Signals
 -  Invocation of the signal handler is called catching the signal.
 -  We use this term interchangeably with handling the signal the user-specified signal handler executes in the context of the affected process
 -  The kernel calls the user-specified handler, and passes control to user space.  When the handler is done (call returns), control returns to the kernel.
 -  Note: the next time the process executes, it will resume back at the instruction where the process was originally interrupted/signaled

# Signals
 -  Portable signal handling:
 -  sigaction () is a standard POSIX API for signal handling allows users on Posix-compliant systems such as Linux and Solaris to specify the signal-handling semantics they want, e.g. whether sys call is aborted or restarted
 -  Is more advanced/expressive than the signal () function
 -  sigaction (signum, struct sigaction\*act, struct sigaction \*oldact)
 -  each struct sigaction can define a handler, e.g. action.sa\_handler = handler;
 -  use sigaction() to define the handler
 -  Also need to set up the timer – could use setitimer instead of alarm.  A SIGALRM is delivered when timer expires.

# Signals
 -  More generally, when a process catches a signal of type signum=k, the handler installed for signal k is invoked with a single integer argument set to k
 -  This argument allows the same handler function to catch different types of signals.

# Signaling Example
 -  A process can send SIGALRM signals to itself by calling the alarm function
 -  alarm(T seconds) arranges for kernel to send a SIGALRM signal to calling process in T seconds
 -  see code example next slide
 -  uses signal function to install a signal handler function that is called asynchronously, interrupting the infinite while loop in main, whenever the process receives a SIGALRM signal
 -  When handler returns, control passes back to main, which picks up where it was interrupted by the arrival of the signal, namely in its infinite loop

# Signaling Example
#include <signal.h>
int beeps=0;
void handler(int sig) {
    if (beeps<5) {
        alarm(3);
        beeps++;
    } else {
        printf("DONE\n");
        exit(0);
    }
}
int main() {
    signal(SIGALRM, handler);
    alarm(3);
    while(1) { ; }
    exit(0);
}

# Signals
 -  a process can selectively block the receipt of certain signals using the function sigprocmask()
 -  when a signal is blocked, it can be delivered, but the resulting pending signal will not be received until the process unblocks the signal.  Like masking interrupts.
 -  A signal that has been sent but not yet received is called a pending signal.
 -  Use sigpending () to get a list of pending signals.
 -  At any point in time, there can be at most one pending signal of a particular type (signal number)
 -  A pending signal is received at most once

# Signals
 -  For each process, the kernel maintains the set of pending signals in the pending bit vector, and the set of blocked signals in the blocked bit vector
# Signals
 -  Handling multiple signals choose to handle the lower number signals first pending signals are blocked, e.g. if a 2 nd SIGINT is received while handling 1 st SIGINT, the 2 nd SIGINT becomes pending and won't be received until after the handler returns pending signals are not queued there can be at most one pending signal of type k, e.g. if a 3 rd SIGINT arrives while the 1 st SIGINT is being handled and the 2 nd SIGINT is already pending, then the 3 rd SIGINT is dropped
 -  system calls can be interrupted
 -  slow system calls that are interrupted by signal handling may not resume in some systems, and may return immediately with an error

# Chapter 5: Synchronization
# Synchronization
 -  Protect access to shared common resources, e.g. buffers, variables, files, devices, etc., by using some type of synchronization
 -  Saw the need for synchronization earlier:
 -  2 processes P1 and P2 use IPC shared memory to modify the same shared memory variable
 -  2 threads T1 and T2 modify the same global or heap variables in same address space, so need thread-safe code
 -  Normal and exceptional control flow both try to modify the same global variable
 -  Producer-Consumer model

# Producer-Consumer Model
 -  a Producer process P1 and a Consumer process C1 share some memory, say a bounded buffer
 -  Producer writes data into shared memory, while Consumer reads data
# Producer-Consumer Model
 -  Track buffer level with a variable
 -  counter
 -  Increment counter when new data is produced/written
 -  Decrement counter when data is consumed/read
 -  keeps track of how much new data is in the buffer that has not yet been read
 -  if counter==0, then consumer can't read from the buffer
 -  if counter==MAX\_BUFF\_SIZE, then producer can't write to the buffer
# Synchronization
# Synchronization
 -  counter++; can compile into several machine language instructions, e.g.

 -  reg1 = counter;
 -    reg1 = reg1 + 1;
 -    counter = reg1;
# A Race Condition Example
 -  // counter++
 -    reg1 = counter;
 -    reg1 = reg1 + 1;
 -    counter = reg1;
# Critical Section
 -  Some kernel data structures could be subject to race conditions, e.g. access to list of open files
 -  Kernel developer must ensure that no such race conditions occur
 -  User or kernel developer identifies
 -  critical sections
 -  in code where each process accesses shared variables to critical sections is controlled by special _entry_ and _exit_ code
# Critical Section
 -  Critical section access should satisfy multiple properties
 -  mutual exclusion process Pi is executing in its critical section, then no other processes can be executing in their critical sections
 -  progress no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in the decision on which will enter its critical section next selection cannot be postponed indefinitely (OS must make a decision eventually, hence "progress")
 -  bounded waiting exists a bound, or limit, on the number of times other processes can enter their critical sections after a process X has made a request to enter its critical section and before that request is granted (no starvation)
 -  For the rest of this chapter, we will primarily be focused on how to achieve mutual exclusion this

# Chapter 5: Mutual Exclusion and Semaphores
# Synchronization
# A Race Condition Example
 -  // counter++
 -    reg1 = counter;
 -    reg1 = reg1 + 1;
 -    counter = reg1;
# Critical Section
 -  Some kernel data structures could be subject to race conditions, e.g. access to list of open files
 -  Kernel developer must ensure that no such race conditions occur
 -  User or kernel developer identifies critical sections in code where each process accesses shared variables to critical sections is controlled by special _entry_ and _exit_ code
# Critical Sections Need Mutual Exclusion
 -  How do we protect access to critical sections?  -  want to prevent another process from executing while current process is modifying this variable - mutual exclusion
 -  Approaches to mutual exclusion: User space – see Peterson's solution Chapter 5.2 Hardware-based - disable interrupts before entering critical section, disable interrupts after exiting critical section, reenable interrupts
 -  This provides mutual exclusion
# Disabling Interrupts
# Disabling Interrupts (2)
# Disabling Interrupts (3)
# Locks
 -  Goal: achieve mutual exclusion without the drawbacks of fully disabling interrupts
## Create a variable/flag called a lock to
 -  The lock's value indicates whether the critical section (or access to its protected variables) is busy
 -  Unset the lock or flag when the critical section is done
## Locks are often called mutexes
# Lock Semantics
 -  Each task calls Acquire(lock) before entering its critical section.
 -  If a task successfully grabs the lock, this task can proceed to execute its critical section
 -  If the lock is not available, the calling task blocks.  So only 1 task at a time has the lock.
 -  Multiple tasks can block waiting on a lock, forming a FIFO queue
 -  A task calls Release(lock) when done, so other processes can enter their critical sections too
 -  1 waiting task is woken (based on order) at each lock release

# A Lock Example
# Locks
 -  Additional advantages:
 -  user doesn't have control over disabling/reenabling interrupts – the OS does this.
 -  user doesn't have to remember to reenable interrupts
 -  So tasks with I/O that need to process interrupts can make progress
 -  Disadvantage:
 -  user still must remember to release(lock), otherwise other tasks blocked on the lock will wait forever
# Test-and-Set Instruction
 -  CPU hardware provides an uninterruptible hardware instruction
 -  TS , which atomically performs both the test and the set operations called by the TestandSet() system call the boolean TestandSet() operation in the next slide is essentially a swap of values
 -  The x86 CPU instruction set contains atomic instructions such as XCHG that are essentially swap statements
 -  Can use atomic XCHG to implement spinlocks

# A Test-and-Set Lock Implementation
 -  Mutual exclusion is achieved - no race conditions
 -  If one process X tries to obtain the lock while another process Y already has it, X will wait in the loop
 -  If a process is testing and/or setting the lock, no other process can interrupt it
 -  Efficient:
 -  The system is exclusively occupied for only a short time - the time to test and set the lock, and not for entire critical section
 -  typically only about 10 instructions
# A Test-and-Set Lock Implementation
 -  Advantage:
 -  Compared to spinning inside the Acquire() system call, this approach spins in user space
 -  Allows the user task to do other execution while polling the TestandSet()
 -  Disadvantage:
 -  requires user to remember to put in while()
 -  while() means busy-waiting, i.e. this is a form of spinlock too except in user space
 -  requires special hardware support in the form of a low level atomic machine instruction TS

# Blocking on Locks
 -  Simplicity of spinlocks:
 -   don't require any other OS constructs, such as a queue of tasks blocked on the lock
 -  If multiple tasks are blocked on the lock, they all spin
 -  Can be useful if blocking time is short
 -  Locks can be augmented with queues to hold blocked tasks
 -  OS scheduler must be involved
 -  Useful when blocking time is long
 -  We'll see an example of this with semaphores

# Semaphores
 -  more general solution to mutual exclusion proposed by Dijkstra
 -  Semaphore S is an integer variable that, apart from initialization, is accessed only through 2 standard atomic operations P(), also called wait()
 -  somewhat equivalent to a test-and-set, but also decrements the value of S V(), also called signal() increments the value of S
 -  OS provides ways to create and manipulate semaphores atomically

# Semaphores
 -  Pseudo-code for classic semaphore
 -  its value can't go below zero, i.e. classic semaphore is non-negative

# A Semaphore Implementation
 -  Based only disabling and reenabling of interrupts
 -  semaphores can also be implemented using TestandSet() instructions

# Binary Semaphores
 -  The previous example showed how to use the semaphore as a binary semaphore its initial value was set to 1 a binary semaphore is also called a mutex lock , i.e. it can be used to provide mutual exclusion on some piece of critical code
 -  Let's define a binary semaphore as a semaphore whose value can't exceed 1, even if many processes keep signal()'ing the semaphore
 -  Additional logic would have to be added to the counting semaphore in its V()/signal() function to cap its maximum value to 1

# Counting Semaphores
 -  A semaphore can also be used more generally as a counting semaphore its initial value is n, e.g. n=10 the value of the semaphore could be used to keep track of the number of instances of a finite resource that are still available
 -  We'll see an example later using counting semaphores to solve the general producer/consumer bounded buffer problem

# Enforcing Order with Semaphores
 -  Enforce order of access between 2 processes P1 and P2
 -  P1 contains code C1 and P2 contains code C2
 -  Want to ensure that code C1 executes before code C2
 -  Use semaphores to synchronize the order of execution of the two processes
 -  if P1 executes 1 st , then C1 will execute 1st, then P1 V()'s semaphore, adding 1 to its value Later, when P2 executes, it will call wait(S), which will decrement the semaphore to 0 – no waiting - followed by execution of C2 Thus C1 executes before C2
 -  If P2 executes 1 st , then P2 blocks on semaphore (=0), so C2 will not be executed yet Later, when P1 executes, it runs C1, then V()'s the semaphore This awakens P2, which then executes C2 Thus C1 executes before C2

# A Revised Semaphore Definition
 -  Efficiently sleep the process until it needs to be woken up by a V()/signal(), rather than spinlock

# A Revised Semaphore Definition
 -  New definition allows a semaphore's value to be negative, because the decrement occurs before the test in P()
 -  The absolute value of the semaphore's negative amount can be used to indicate the # of processes blocked on the semaphore
 -  Processes now yield the CPU if the semaphore's value is negative, rather than busy wait
 -  If more than one process is blocked on a semaphore, then use a FIFO queue to select the next process to wake up when a semaphore is V'ed
 -  Why is LIFO to be avoided?

# Mutual Exclusion with Revised Semaphore
 -  Semaphore S = 1;
 -  // initial value of semaphore is 1
 -  int counter;
 -              // assume counter is set correctly somewhere in                                 code
# Mutual Exclusion with Revised Semaphore
 -  Semaphore S = 1;
 -  // initial value of semaphore is 1
 -  int counter;
 -              // assume counter is set correctly somewhere in                                 code

# Enforcing Order with Revised Semaphore
 -  If P1 hits its signal(S) first, before P2 hits wait(S), then
 -  P1 will have executed C1 already, will then increment S to 1, and no process will be unblocked.
 -  Later, P2 calls wait(S), decrements S from 1 to 0, and executes C2.  Order is preserved: C1 executes before C2
 -  If P2 hits wait(S) before P1 hits signal(S), then
 -  P2 will decrement S to -1, and block P2 on the semaphore.
 -  Next, P1 calls signal(S) having executed code C1, incrementing S from -1 to 0 and unblocking P2.
 -  P2 now executes C2.  Order is preserved: C1 executes before C2

# Deadlock
 -  Semaphores provide synchronization, but can introduce more complicated higher level problems like deadlock two processes deadlock when each wants a resource that has been locked by the other process
 -  e.g. P1 wants resource R2 locked by process P2 with semaphore S2, while P2 wants resource R1 locked by process P1 with semaphore S1

# Deadlock Example
** Semaphore Q= 1;    **
 -  // binary semaphore as a mutex lock
** Semaphore S = 1;  **
 -  // binary semaphore as a mutex lock
**variable R1, R2;**

# Deadlock
 -  In the previous example, Each process will sleep on the other process's semaphore the V() signalling statements will never get executed, so there is no way to wake up the two processes from within those two processes there is no rule prohibiting an application programmer from P()'ing Q beefore S, or vice versa - the application programmer won't have enough information to decide on the proper order in general, with N processes sharing N semaphores, the potential for deadlock grows

# Other Deadlock Examples
 -  A programmer mistakenly follows a P() with a second P() instead of a V(), e.g.
 -  P(mutex)
 -      critical section
 -  P(mutex)
 -  This causes a self-deadlock!
 -  A programmer forgets and omits V(mutex).  Can cause deadlock.
 -  P1:                                P2:
 -  P(mutex)
 -                          P(mutex)
 -      critical section 1                    critical section 2
 -  V(mutex)
 -  P2 calls P(mutex) and executes crit sect 2.  Then P1 blocks on P(mutex), then P2 blocks on P(mutex)
 -  A programmer forgets and omits P(mutex). Can violate mutual exclusion if P(mutex) is omitted.
 -  P1:                                P2:
 -  P(mutex)
 -      critical section 1                    critical section 2
 -  V(mutex)
 -                          V(mutex)
 -  A programmer reverses the order of P() and V(), e.g.
 -  V(mutex)
 -      critical section    <---- this violates mutual exclusion,
 -  P(mutex)                        but is not deadlock

# Chapter 5: Classic Synchronization Problems

# A Revised Semaphore Definition
 -  Efficiently sleep the process until it needs to be woken up by a V()/signal(), rather than spinlock

# Classic Synchronization Problems
 -  Bounded Buffer Producer-Consumer Problem
 -  Readers-Writers Problem
 -  First Readers Problem
 -  Dining Philosophers Problem
 -  These are not just abstract problems
 -  Represent classes of synchronization problems encountered in the real world when trying to synchronize access to shared resources among multiple processes or threads

# Prior Bounded-Buffer P/C Approach
 -  while(1) {
 -      while(counter==MAX);
 -      buffer[in] = nextdata;
 -      in = (in+1) % MAX;

 -  Acquire(lock);
 -      counter++;
 -      Release(lock);
 -  }

# Bounded-Buffer P/C Goals
 -  In the prior approach, both the producer and consumer are busy-waiting
 -  Instead, want both to sleep as necessary
 -  Goal #1: Producer should block when buffer is full
 -  Goal #2: Consumer should block when the buffer is empty
 -  Also, Goal #3: mutual exclusion when buffer is partially full
 -  Producer and consumer should access the buffer in a synchronized mutually exclusive way

# Bounded-Buffer P/C Design
 -  To achieve Goal #3 of mutual exclusion: Use a mutex semaphore to protect access to buffer manipulation, mutex init = 1
 -  To achieve Goal #1, producer blocks if buffer full:
 -  Use a counting semaphore called empty that is initialized to empty init = MAX
 -  Each time the producer adds an object to the buffer, this decrements the # of empty slots, until it hits 0 and the producer blocks

# Bounded-Buffer P/C Design
 -  To achieve Goal #2, consumer blocks if buffer empty:
 -  Define a counting semaphore full that is initialized to full init = 0 full tracks the # of full slots and is incremented by the producer
 -  Each time the consumer removes a full slot, this decrements full , until it hits 0, then the consumer blocks

# The Readers/Writers Problem
 -  N tasks want to write to a shared file
 -  M other tasks want to read from same shared file
 -  Must synchronize access

# R/W vs P/C Comparison
# 1 st and 2 nd Readers/Writers Problems
 -  1 st R/W Problem:
 -  Clarification to Goal #2: no reader is kept waiting unless a writer already has seized the shared object.
 -  2 nd R/W Problem:
 -  Caveat to 1 st R/W: a pending writer should not be kept waiting indefinitely by readers that arrived after the writer
 -  i.e. a pending writer cannot starve

# Readers/Writers Problem
 -  Note that the 1 st R/W problem gave precedence to readers new readers can keep arriving while any one reader holds the write lock, which can starve writers until the last reader is finished
 -  Instead, allow a pending writer to block future reads
 -  This way, writers don't starve.
 -  If there is a writer,
 -  New readers should block
 -  Existing readers should finish then signal the waiting writer

 -  Original Solution to the 2 nd Readers/Writers Problem
# 2 nd Readers/Writers Starvation
 -  Once 1 st writer grabs readBlock, any number of writers can come through while the 1 st reader is blocked on redBlock and subsequent readers are blocked on writePending
 -  So, behavior is that a writer can block not just new readers, but also some earlier readers
 -  Note now that readers can be starved!
 -  Instead, want a solution that is starvation-free for both readers and writers

# Dining Philosophers Problem
 -  N philosophers seated around a circular table
 -  There is one chopstick between each philosopher
 -  A philosopher must pick up its two nearest chopsticks in order to eat
 -  A philosopher must pick up first one chopstick, then the second one, not both at once
 -  Devise an algorithm for allocating these limited resources (chopsticks) among several processes (philosophers) in a manner that is deadlock-free, and starvation-free
# Dining Philosophers Problem
 -  A simple algorithm for protecting access to chopsticks:
 -  Access to each chopstick is protected by a mutual exclusion semaphore prevents any other philosopher from picking up the chopstick when it is already in use by a philosopher

# Dining Philosophers Problem
 -  Pseudo code for Philosopher i:

**while(1) {**
**    // obtain 2 chopsticks to my**
**       immediate right and left**
**    P(chopstick[i]);**
**    P(chopstick[(i+1)%N];**
**    // eat**
**    // release both chopsticks**
**    V(chopstick[(i+1)%N];**
**    V(chopstick[i]);**
**}**

# Dining Philosophers Problem
 -  Unfortunately, the previous "solution" can result in deadlock each philosopher grabs its right chopstick first causes each semaphore's value to decrement to 0
 -  each philosopher then tries to grab its left chopstick each semaphore's value is already 0, so each process will block on the left chopstick's semaphore
 -  These processes will never be able to resume by themselves - we have deadlock!

# Dining Philosophers Problem
 -  Deadlock-free solutions?
 -  allow at most 4 philosophers at the same table when there are 5 resources
 -  odd philosophers pick first left then right, while even philosophers pick first right then left
 -  allow a philosopher to pick up chopsticks only if both are free.
 -  This requires protection of critical sections to test if both chopsticks are free before grabbing them.
We'll see this solution next using monitors
Also, there is a construct called an AND semaphore
 -  A deadlock-free solution is not necessarily starvation-free for now, we'll focus on breaking deadlock

# Monitors
 -  semaphores can result in deadlock due to programming errors
 -  forgot to add a P() or V(), or misordered them, or duplicated them
 -  to reduce these errors, introduce high-level synchronization primitives, e.g.
 -  monitors with condition variables ,
 -  essentially automates insertion of P and V for you
 -  As high-level synchronization constructs, monitors are found in high-level programming languages like Java and C
 -  underneath, the OS may implement monitors using semaphores and mutex locks

# Monitors
 -  Declare a monitor as follows (looks somewhat like a C++ class):

    **monitor** _monitor\_name_ {
      // shared local variables
      **function** f1(...) {
      ...
      }
      ...
      **function** fN(...) {
      ...
      }
      **init\_code** (...) {
      ...
      }
   }

# Monitors
 -  Declare a monitor as follows (looks somewhat like a C++ class):

    **monitor** _monitor\_name_ {
      // shared local variables
      **function** f1(...) {
      ...
      }
      ...
      **function** fN(...) {
      ...
      }
      **init\_code** (...) {
      ...
      }
   }
# Supplementary Slides
# Details on the 2 nd R/W Problem
Comparing the solution of the 2nd R/W problem to the solution for the 1st R/W problem:
The reader has not changed much from the 1st R/W problem, just adding logic to block new readers if there's a pending writer
Writer has changed substantially, but it resembles the reader in the 1st R/W problem, i.e. the first writer that arrives blocks all future readers, multiple writers are allowed in (but synchronized one at a time using writeBlock), and the last writer out starts activating the readers
Scenario for reasoning through the solution:
suppose there are multiple "current" readers - they could all be in read(resource)
then the 1st writer arrives.
This 1st writer blocks future reads by setting readBlock, then blocks on writeBlock, waiting for current readers to finish
Subsequent writers also block on writeBlock.
Then a new reader arrives - it blocks on readBlock.
All subsequent new readers will block on writePending.
Once the current readers finish, the last current reader will awaken the 1st writer on writeBlock by V'ing writeBlock
Multiple writers can now arrive and be serviced in synchronized order
they will continue to block all new readers – starving new readers…
… until the last writer finishes, and V's readBlock, releasing the 1st new reader, who will V(writePending) and release the 2nd new reader, etc., eventually freeing up all multiple readers so that they can again execute in read(resource)
# Details on the 2 nd R/W Problem
 -  writePending is an optimization in the previous example, because without it, all readers would block on readBlock, and a new writer would not be able to quickly gain access to the shared file over pending Readers
Want the behavior to be that if a new writer comes along, that stops all future reads, _including_ currently blocked readers who have not yet entered their reading critical sections
Without writePending, all readers would block on readBlock, waiting for a writer to finish.  Once that writer finishes, the 1st reader proceeds into its critical section.  If another writer comes in before the 2nd queued reader can get started, then want writer to block the 2nd reader.  Without writePending, the 1st reader will wake up the 2nd reader blocked on readBlock, who will then wake up the 3rd reader blocked on readBlock, emptying readBlock before the writer (who is blocked on readBlock but is at the end of the FIFO sleep queue) gets access to its critical section
Instead, want a new writer to stop queued Readers from proceeding.  With writePending, if there are multiple readers waiting for writer #1 to finish, then 1st reader blocks on readBlock, and the 2nd reader, and 3rd, 4th, … will all block on writePending.  When writer #1 is finished, reader #1 proceeds, and let's say reader #2 as well.  But if writer #2 arrives, it will grab readBlock, which will block reader #3 from proceeding, i.e. reader #3 will block on readBlock, while readers #4, #5, … are still stuck on writePending.  In this way, writer #2 is able to proceed more quickly to gain access the shared file.

# Chapter 5: Condition Variables, Monitors
# Monitors
 -  Declare a monitor as follows (looks somewhat like a C++ class):

    **monitor** _monitor\_name_ {
      // shared local variables
      **function** f1(...) {
      ...
      }
      ...
      **function** fN(...) {
      ...
      }
      **init\_code** (...) {
      ...
      }
   }
# Monitor Example
**monitor** Account {
private int **balance** ;  // shared var requiring mutual exclusion
public function **deposit** (int amount) {
    balance = balance + amount;
}
public function **withdraw** (int amount) {
    balance = balance - amount;
}
private init() { balance=0; }
}
# Condition Variables
 -  Augment the mutual exclusion of a monitor with an ordering mechanism
 -  Recall: Semaphore P() and V() provide both mutual exclusion and ordering
 -  Monitors alone only provide mutual exclusion
 -  A condition variable provides ordering
 -  Used when one task wishes to wait until a condition is true before proceeding
 -  Such as a queue being full enough or data being ready
 -  A 2 nd task will signal the waiting task, thereby waking up the waiting task to proceed

# Condition Variables
 -  A condition variable x allows primarily two main operations on itself: x.wait()  --  suspends the calling task
 -  Can have many processes blocked typically released in FIFO order textbook describes another variation specifying a priority p, i.e. call x.wait(p)
 -  x.signal() -- resumes exactly 1 suspended task.  If none, then no effect

 -  Declare a condition variable with pseudo-code:
 -       condition x,y;
# Condition Variables Example
 -  Block Task 1 until a condition holds true, e.g. queue is empty

condition wait\_until\_empty;

# Condition Variables vs Semaphores
 -  Both have wait() and signal(), but semaphore's signal()/V() preserves state in its integer value

Semaphore wait\_until\_empty=0;

# Complex Conditions
 -  Suppose you want task T1 to wait until a complex set of conditions set by T2 becomes TRUE use a condition variable
 -  Surround the test of the conditions with a mutex to atomically test the set of conditions

# Broadcast Signals
 -  In some cases, you may want to wake all tasks blocked on a condition variable x, not just one
 -  x.signal() only wakes one task x.broadcast() is a 3 rd operation provided for CVs on some systems
 -  Wakes all tasks blocked on a CV
 -  In pthreads, pthread\_cond\_broadcast() wakes all waiting threads

# Monitors and Condition Variables
**  monitor MON** {
      condition x;
      // shared local variables
      **function** f1(...) {
      ...
        x.wait();
      ...
      }
      **function** f2(...) {
      ...
        x.signal();
      ...
      }
      **init\_code** (...) {
      ...
      }
   }

# Hoare vs Mesa Semantics
 -  Hoare semantics, also called signal-and-wait
 -  The signaling process P1 either waits for the woken up process P2 to leave the monitor before resuming, or waits on another CV
 -  Mesa semantics, also called signal-and-continue
 -  The signaled process P2 waits until the signaling process P1 leaves the monitor or waits on another condition

# Monitor-based Solution to Dining Philosophers
 -  Key insight: pick up 2 chopsticks only if both are free
 -  this avoids deadlock
 -  reword insight: a philosopher moves to his/her eating state only if both neighbors are not in their eating states
 -  thus, need to define a state for each philosopher

# Monitor-based Solution to Dining Philosophers (2)
 -  2nd insight: if one of my neighbors is eating, and I'm hungry, ask them to signal() me when they're done
 -  thus, states of each philosopher are: thinking, hungry, eating
 -  thus, need condition variables to signal() waiting hungry philosopher(s)
 -  Also need to Pickup() and Putdown() chopsticks

# Monitor-based Solution to Dining Philosophers (3)
 -  monitor DP {
 -        status state[5];
 -        condition self[5];
 -        Pickup(int i);
 -        Putdown(int i);
 -             test();
 -             init();
 -     }

# Monitor-based Solution to Dining Philosophers (4)
monitor DP {
   status state[5];
   condition self[5];

# Monitor-based Solution to Dining Philosophers (5)
... monitor code continued from previous slide...
   ...
Putdown(int i) {
      state[i] = thinking;
      test((i+1)%5);
      test((i-1)%5);
   }

   init() {
      for i = 0 to 4
         state[i] = thinking;
   }
}  // end of monitor

# Complete Monitor-based Solution to Dining Philosophers
monitor DP {
   status state[5];
   condition self[5];

# DP Monitor Deadlock Analysis
Try various scenarios to verify for yourself that deadlock does not occur in them
Start with one philosopher P1
Now suppose P2 arrives to the left of P1 while P1 is eating
What is the perspective from P1?
What is the perspective from P2?
Now supposes P5 arrives to the right of P1 while P1 is eating and P2 is waiting
Perspective from P1?
Perspective from P5?
Perspective from P2?
Now suppose P4 arrives to the right of P5 while P2 and P5 are waiting and P1 is eating
Perspective from P5?
Perspective from P4?
Perspective from P1?
Suppose P2 arrives while both P1 and P3 are eating
If P1 finishes first, it can't wake up P2
But when P3 finishes, its call to test(P2) will wake up P2, so no deadlock
Suppose there are 6 philosophers and the evens are eating.  How do the odds get to eat?

# DP Monitor Solution
 -  Note that starvation is still possible in the DP monitor solution
 -  Suppose P1 and P3 arrive first, and start eating, then P2 arrives and sets its state to hungry and blocks on its CV
 -  When P1 and P3 end, they will call test(P2), but nothing will happen, i.e. P2 won't be signaled because the signal only occurs inside the if statement of test, and the if condition is not satisfied
 -  Next, P1 and P3 can eat again, repeatedly, starving P2

# DP Solution Analysis
 -  Signal() happening before the wait() doesn't matter
 -  Signal() in Pickup() has no effect the 1 st time
 -  Signal() called in  Putdown() is the actual wakeup

# Chapter 6: Scheduling
# Switching Between Processes
 -  A process can be switched out due to:
 -  blocking on I/O
 -  voluntarily yielding the CPU, e.g. via other system calls
 -  being preemptively time sliced, i.e interrupted
 -  Termination

# Switching Between Processes
 -  the dispatcher gives control of CPU to the process selected by the scheduler, causing context switch:
 -  save old state
 -  select next process
 -  load new state
 -  switch to user mode, jumping to the proper location in the user process to restart that process
 -  Separate the mechanism of scheduling from the policy of scheduling

# Context Switch Overhead
 -  Typically take 10 microseconds to copy register state to/from memory
 -  on a 1 GHz CPU, that's 10000 wasted cycles per context switch!
 -  if the time slice is on the order of a context switch, then CPU spends most of its time context switching
 -  Typically choose time slice to be large enough so that only 10% of CPU time is spent context switching
 -  Most modern systems choose time slices of 10-100 ms

# Scheduling Definitions
 -  execution time E(P i ) = the time on the CPU required to fully execute process i
 -  Sum up the time slices given to process i
 -  Also called the "burst time" by textbook

# Scheduling Definitions
 -  wait time W(P i ) = the time process i is in the ready state/queue waiting but not running
 -  Sum up the gaps between time slices given to process i, but doesn't include I/O waiting time

# Scheduling Definitions
 -  turnaround time T(P i ) = the time from 1 st entry of process i into the ready queue to its final exit from the system (exits last run state)

# Scheduling Definitions
 -  response time
 -  R(P i ) = the time from 1 st entry of process i into the ready queue to its 1 st scheduling on the CPU (1 st run state)

# Scheduling Criteria
 -  Scheduler's job is to decide the next process (or kernel thread) to run
 -  From among the set of processes/kernel threads in the ready queue
 -  Scheduler implements a scheduling policy , that may adhere to one or more of the following goals:
 -  maximize CPU utilization: 40% to 90%
 -  maximize throughput: # processes completed/second
 -  minimize average or peak turnaround time: how long it takes to finish executing a process from 1 st entry to final exit
 -  min ave/peak waiting time: sum of time in ready queue
 -  min ave/peak response time: time until first response
 -  Some processes can generate early results, so if they get some CPU time quickly, they can start producing output sooner.  A quick response time from the scheduler benefits such processes.
 -  maximize fairness
 -  meet deadlines or delay guarantees
 -  ensure priorities are adhered to

# Scheduling Analysis
 -  We analyze various scheduling policies to see how efficiently they perform with respect to metrics like:
 -  Wait time, turnaround time, response time, etc.
 -  Some algorithms will be optimal in certain metrics
 -  To simplify analysis assume:
 -  No blocking I/O.  Focus only on scheduling processes/tasks that have provided their execution times
 -  Processes execute until completion, unless otherwise noted, e.g round robin.

# FCFS Scheduling
 -  First Come First Serve: order of arrival dictates order of scheduling
 -  Nonpreemptive, processes execute until completion
 -  If processes arrived in order P1, P2, P3 before time 0, then
 -  Gantt chart of CPU service time is:
 -  If processes arrive in reverse order P3, P2, P1 around time 0, then Gantt chart of CPU service time is:

 -  Case I: average wait time is (0+24+27)/3 = 17 seconds
 -  Case II: average wait time is (0+3+6)/3 = 3 seconds
 -  FCFS wait times are generally not minimal - vary a lot if order of arrival changed, which is especially true if the process service times vary a lot (are spread out)

 -  Case I: average turnaround time is (24+27+30)/3 = 27 seconds
 -  Case II: average turnaround time is (3+6+30)/3 = 13 seconds
 -  A lot of variation in turnaround time too.

# Shortest Job First (SJF) Scheduling
 -  Choose the process/thread with the lowest execution time
 -  gives priority to shortest or briefest processes
 -  minimizes the average wait time
 -  intuition: moving a long process before a short one increases the wait time of short processes a lot.
 -  Conversely, moving long process to the end decreases wait time seen by short processes
 -  Also, the impact of the wait time on long processes moved towards the end is minimal

# Shortest Job First (SJF) Scheduling
 -  It has been proved that SJF minimizes the average wait time out of all possible scheduling policies. Sketch of proof:

 -  Given a set of processes {P a , P b , …, P n }, suppose one chooses a process P from this set to schedule first.
 -  The wait times for all the remaining processes in {P a , …, P n }-P will be increased by the run time of P.
 -  If P has the shortest run time (SJF), then the wait times will increase the least.

# Shortest Job First (SJF) Scheduling
 -  Sketch of proof (continued):

 -  Apply this reasoning iteratively to each remaining subset of processes.
 -  At each step, the wait time of the remaining processes is increased least by scheduling the process with the smallest run time.
 -  The average wait time is minimized by minimizing each process' wait time,
 -  Each process' wait time is the sum of all earlier run times, which is minimal if the shortest job is chosen at each step above.

# Shortest Job First Scheduling
 -  In this example, P1 through P4 are in ready queue at time 0:
 -  can prove SJF minimizes wait time
 -  - out of 24 possibilities of ordering P1 through P4, the SJF ordering has the lowest average wait time

# Shortest Job First Scheduling
 -  Problem?  -  must know run times E  p i  in advance unlike FCFS
 -  Solution: estimate CPU demand in the next time interval from the process/thread's CPU usage in prior time intervals
 -  Divide time into monitoring intervals, and in each interval n, measure the CPU time each process Pi takes as CPU(n,i).
 -  For each process Pi, estimate the amount of CPU time EstCPU(n,i) for the next interval as the average of the current measurement and the previous estimate

# Shortest Job First Scheduling
 -  Solution (continued):
 -  EstCPU(n+1,i) =  \*CPU(n,i) + (1-  )\*EstCPU(n,i)      where 0<  <1
 -  If  >1/2, then estimate is influenced more by recent history.  If  <1/2, then bias the estimate more towards older history
 -  This kind of average is called an exponentially weighted average.
 -  See textbook for more.

# Shortest Job First Scheduling
 -  Can be preemptive:
 -  i.e. when a new job arrives in the ready queue, if its execution time is less than the currently executing job's remaining execution time, then it can preempt the current job
 -  For simplicity, we assumed in the preceding analysis that jobs ran to completion and no new jobs arrived until the current set had finished.
 -  Compare to FCFS: a new process can't preempt earlier processes, because its order is later than the earlier processes

# Scheduling: Round Robin, Deadlines, Priorities, Multi-level Feedback Queues

# First Come First Serve (FCFS) Scheduling
 -  Tasks are scheduled according to the order they arrive
 -  Simple to implement
 -  Can result in high variance

# Shortest Job First Scheduling
 -  Schedule tasks with the shortest execution times first
 -  Can prove this results in the lowest average wait time

# Round Robin Scheduling
 -  Assume preemptive time slicing
 -  A task is forced to relinquish the CPU before it's necessarily done
 -  Periodic timer interrupt transfers control to the CPU scheduler, which rotates among the processes in the ready queue, giving each a time slice, e.g. if there are 3 tasks T1, T2, & T3, then the scheduler will keep rotating among the three: T1, T2, T3, T1, T2, T3, T1, …
 -  treats the ready queue as a circular queue
 -  Example: let time slice = 4 ms useful to support interactive applications in multitasking systems hence is a popular scheduling algorithm
 -  Properties:
 -  Simple to implement: just rotate, and don't need to know execution times a priori
 -  Fair: If there are n tasks, each task gets 1/n of CPU
 -  A task can finish before its time slice is up
 -  Scheduler just selects the next task in the queue

# Weighted Round Robin
 -  Give some some tasks more time slices than others
 -  This is a way of implementing priorities – higher priority tasks get more time slices per round
 -  If task T i gets N i slots per round, then the fraction  i of the CPU bandwidth that task i gets is:

# Weighted Round Robin
 -  In previous example, could give T1 2 time slices, and T2 and T3 only 1 each round

# Deadline Scheduling
 -  Hard real time systems require that certain tasks must finish executing by a certain time, or the system fails

# Earliest Deadline First (EDF) Scheduling
 -  Choose the task with the earliest deadline
 -  This task most urgently needs to be completed

# Deadline Scheduling
 -  Even EDF may not be able to meet all deadlines:
 -  In previous example, if T3's deadline was t=9, then EDF cannot meet T3's deadline
 -  When EDF fails, the results of further failures, i.e. missed deadlines, are unpredictable
 -  Which tasks miss their deadlines depends on when the failure occurred and the system state at that time
 -  Could be a cascade of failures
 -  This is one disadvantage of EDF
 -  Admission control policy
 -  Check on entry to system whether a task's deadline can be met,
 -  Examine the current set of tasks already in the ready queue and their deadlines
 -  If all deadlines can be met with the new task, then admit it.  The schedulability of the set of real-time tasks has been verified.
 -  Else, deny admission to this task if its deadline can't be met.
 -  Note FCFS, SJF and priority had no notion of refusing admission

# EDF and Preemption
 -  Assume a preemptively time sliced system
 -  A task arriving with an earlier deadline can preempt one currently executing with a later deadline.

# EDF and Preemption
 -  At time 0, tasks T0 and T1 have arrived.  EDF chooses T0.
 -  At time 2, preempt T1.  EDF chooses newly arrived T2 with earlier deadline.
 -  At time 4, T2 finishes and makes deadline.  EDF chooses T1.

# Deadline Scheduling
 -  There are other types of deadline schedulers
 -  Example: a Least Slack algorithm chooses the task with the smallest slack = time until deadline – remaining execution time
 -  i.e. slack is the maximum amount of time that a task can be delayed without missing its deadline
 -  Tasks with the least slack are those that have the least flexibility to be delayed given the amount of remaining computation needed before their deadline expires
 -  Both EDF and Least Slack are optimal according to different criteria

# Soft Real Time Systems
 -  Soft real time systems seek to meet most deadlines, but allow some to be missed
 -  Unlike hard real time systems, where every deadline must be met or else the system fails
 -  Soft real time scheduler may seek to provide probabilistic guarantees
 -  e.g. if 60% of deadlines are met, that may be sufficient for some systems
 -  Linux supports a soft real-time scheduler based on priorities – we'll see this next

# Priority-based Scheduling
 -  Assign each task a priority, and schedule higher priority tasks first, before lower priority tasks
 -  Any criteria can be used to decide on a priority measurable characteristics of the task external criteria based on the "importance" of the task
 -  Example: foreground processes may get high priority, while background processes get low priority
 -  Can be preemptive:
 -  A higher priority process arriving in the ready queue can preempt a lower priority running process
 -  Can occur if the lower priority process …:
 -  Yields CPU with a system call
 -  Is interrupted by a timer interrupt
 -  Is interrupted by a hardware interrupt
 -  Each of these cases gives control back to the OS, which can then schedule the higher priority process

# Priority-based Scheduling
 -  Multiple tasks with the same priority are scheduled according to some policy
 -  FCFS, round robin, etc.
 -  Each priority level has a set of tasks, forming a multi-level queue
 -  Each level's queue can have its own scheduling policy
 -  We use priority-based scheduling and multi-level queue scheduling interchangeably

## Multilevel Queue Scheduling
# Priority-based Scheduling
 -  Preemptive priorities can starve low priority processes
 -  A higher priority task always gets served ahead of a lower priority task, which never sees the CPU
 -  Some starvation-free solutions:
 -  Assign each priority level a proportion of time, with higher proportions for higher priorities, and rotate among the levels
 -  Similar to weighted round robin, except across levels
 -  Create a multi-level feedback queue that allows a task to move up/down in priority
 -  Avoids starvation of low priority tasks
## Multilevel Feedback Queue Scheduling
## Multilevel Feedback Queue
Multilevel-feedback-queue scheduler defined by the following parameters:
number of queues
scheduling algorithms for each queue
method used to determine when to upgrade a process
method used to determine when to demote a process
method used to determine which queue a process will enter when that process needs service

# Multi-level Feedback Queues
 -  Criteria for process movement among priority queues could depend upon age of a process:
 -  old processes move to higher priority queues, or conversely, high priority processes are eventually demoted
 -  sample aging policy: if priorities range from 1-128, can decrease (increment) the priority by 1 every T seconds
 -  eventually, the low priority process will get scheduled on the CPU
# Multi-level Feedback Queues
 -  Criteria for process movement among priority queues could depend upon behavior of a process:
 -  could be CPU-bound processes move down the hierarchy of queues, allowing interactive and I/O-bound processes to move up give a time slice to each queue, with smaller time slices higher up if a process doesn't finish by its time slice, it is moved down to the next lowest queue over time, a process gravitates towards the time slice that typically describes its average local CPU burst
## Example of Multilevel Feedback Queue
Three queues:
_Q_0 – RR with time quantum 8 milliseconds
_Q_1 – RR time quantum 16 milliseconds
_Q_2 – FCFS
Scheduling
A new job enters queue _Q__0_which is servedFCFS. When it gains CPU, job receives 8 milliseconds.  If it does not finish in 8 milliseconds, job is moved to queue _Q_1.
At _Q_1 job is again served FCFS and receives 16 additional milliseconds.  If it still does not complete, it is preempted and moved to queue _Q_2.
Interactive processes are more likely to finish early, processing a small amount of data, while compute-bound processes will exhaust their time slice.  So interactive processes will gravitate towards higher priority queues.

# Priority-based Scheduling
 -  In Unix/Linux, you can nice a process to set its priority, within limits
 -  e.g. priorities can range from -20 to +20, with lower values giving higher priority, a process with 'nice +15' is "nicer" to other processes by incrementing its value (which lowers its priority)
 -  E.g. if you want to run a compute-intensive process compute.exe with low priority, you might type at the command line "nice –n 19 compute.exe"
 -  To lower the niceness, hence increase priority, you typically have to be root
 -  Different schedulers will interpret/use the nice value in their own ways

# Multi-level Feedback Queues
 -  In Windows XP and Linux, system & real-time tasks are grouped in a priority range that is higher in priority than the priority range of  non-real-time tasks
 -  XP has 32 priorities.  1-15 are for normal processes, 16-31 are for real-time processes.  One queue for each priority.
 -  XP scheduler traverses queues from high priority to low priority until it finds a process to run
 -  In Linux, priorities 0-99 are for important/real-time processes while 100-139 are for 'nice' user processes.  Lower values mean higher priorities.
 -  Also, longer time quanta for higher priority tasks (200 ms for highest) and shorter time quanta for lower priority tasks (10 ms for lowest).
## Linux Priorities and Time-slice length

# Multi-level Feedback Queues
 -  Most modern OSs use or have used multi-level feedback queues for priority-based preemptive scheduling
 -  e.g. Windows NT/XP, Mac OS X, FreeBSD/NetBSD and Linux pre-2.6
 -  Linux 1.2 used a simple round robin scheduler
 -  Linux 2.2 introduced scheduling classes (priorities) for real-time and non-real-time processes and SMP (symmetric multi-processing) support
 -  Linux 2.4 and above next lecture…

# More Linux Scheduler History
 -  Linux 2.4 introduced an O(N) scheduler – help interactive processes
 -  If an interactive process yields its time slice before it's done, then its "goodness" is rewarded with a higher priority next time it executes
 -  Keep a list of goodness of all tasks.
 -  But this was unordered.  So had to search over entire list of N tasks to find the "best" next task to schedule – hence O(N)
 -  doesn't scale well

# More Linux Scheduler History
 -  Linux 2.6-2.6.23 uses an O(1) scheduler
 -  Iterate over fixed # of 140 priorities to find the highest priority task
 -  The amount of search time is bounded by the # priorities, not the # of tasks.
 -  Hence O(1) is often called "constant time"
 -  scales well because larger # tasks doesn't affect time to find best next task to schedule
# O(1) Scheduler in Linux
 -  Linux maintains two queues:
 -  an active array or run queue and an expired array/queue, each indexed by 140 priorities
 -  Active array contains all tasks with time remaining in their time slices, and expired array contains all expired tasks
 -  O(1) Scheduler in Linux

# O(1) Scheduler in Linux
 -  An expired task is not eligible for execution again until all other tasks have exhausted their time slice
 -  Scheduler chooses task with highest priority from active array
 -  # of steps to find the highest priority task is in the worst case 140
 -  This search is bounded and depends only on the # priorities, not # of tasks, unlike the O(N) scheduler
 -  hence this is O(1) in complexity
 -  When a task is moved from run to expired, Linux recalculates its priority according to a heuristic
 -  New priority = nice value +/- f(interactivity)
 -  f() can change the priority by at most +/-5, and is closer to -5 if a task has been sleeping while waiting for I/O
 -  interactive tasks tend to wait longer times for I/O, and thus their priority is boosted -5, and closer to +5 for compute-bound tasks
 -  This dynamic reassignment of priorities affects only the lowest 40 priorities for non-RT/user tasks (corresponds to the nice range of +/- 20)
 -  The heuristics became difficult to implement/maintain

# Completely Fair Scheduler (CFS) in Linux
 -  Linux 2.6.23+/3.\* has a "completely fair" scheduler
 -  Based on concept of an "ideal" multitasking CPU

# CFS Intuition
 -  On an ideal CPU, N tasks would run truly in parallel, each getting 1/N of CPU and each executing at every instant of time
 -  Example: for a 4 GHz processor, if there are 4 tasks, each gets a 1 GHz processor for each instant of time
 -  Each such task makes progress at every instant of time
 -  This is "fair" sharing of the CPU among each of the tasks
 -  In practice, we know a real (1-core) CPU cannot run N tasks truly in parallel
 -  Only 1 task can run at a time
 -  Time slice in/out the N tasks, so that in steady state each task gets ~ 1/N of CPU
 -  This gives the illusion of parallelism
 -  Thus, what we have is concurrency, i.e. the N tasks run concurrently, but not truly in parallel
 -  Ingo Molnar (designer of CFS):
 -  "CFS basically models an 'ideal, precise multitasking CPU' on real hardware."
 -  So CFS's goal is to approximate an ideally shared CPU
 -  Approach: when a task is given T seconds to execute, keep a running balance of the amount of time owed to other tasks as if they all ran on an ideal CPU
 -  Example: Task T1 is given a T second time slice on the CPU
 -  Suppose there are 3 other tasks T2, T3, and T4
 -  On an ideal CPU, in any interval of time T, then T1, T2, T3 and T4 would each have had the equivalent of time T/4 on the CPU
 -  Instead, on a real CPU T1 is given T instead of T/4, so T1 has been overallocated 3T/4
 -  T2, T3 and T4 are owed time T/4 on the CPU, i.e. they have each been forced to wait the equivalent of T/4

# CFS Intuition
 -  Example:
 -  The current accounting balance is summarized in the table below

# CFS Intuition
 -  Example: let's have round robin over 4 tasks

# CFS Intuition
 -  Suppose a 5 th task T5 is added to the round robin
 -  Now the amount owed/wait time is calculated as T/5 for each task not chosen for a time slice, and as -4T/5 for the chosen task for a time slice
 -  In general, if there are N runnable tasks, then
 -  (N-1)T/N is subtracted from the balance owed/wait time of the chosen task
 -  T/N is added to the balanced owed/wait time of all other ready-to-run tasks
 -  T5 is initially owed no CPU time, so W 5 = 0
 -  Example: If T5 had arrived just after T2's time slice, then T5's wait time =0 would place it above T1 and T2 but below T3 and T4 in terms of amount of time owed on the CPU

# CFS Scheduler in Linux
 -  Goal of CFS Scheduler:
 -  select the task with the longest wait time
 -  i.e. choose max W i
 -  This is the task that is owed the most time on the CPU and so should be run next to achieve fairness most quickly

# Wait Time Calculation
 -  In general, each scheduling decision at time k may choose:
 -  An arbitrary amount of time T(k) to schedule the chosen task, i.e. it doesn't have to be a fixed time slot T
 -  The number of runnable tasks N(k) may change at each decision time k
 -  Total accumulated wait time for each task i at time k is:

# CFS Scheduler in Linux
 -  Recall: CFS scheduler chooses task with max Wtotal i (k) at each scheduling decision k
 -  Maximizing Wtotal i (k) equivalent to minimizing the quantity [Global fair clock - Wtotal i (k)] 1 st
 -  CFS scheduler:
 -  Had to track global fair clock and Wtotal i (k) for each task i
 -  Then would compute the values [Global fair clock - Wtotal i (k)]
 -  Then ordered these values in a Red-Black tree
 -  Then selected leftmost node in tree (has minimum value) and scheduled the task corresponding to this node

# CFS Scheduler in Linux
 -  Revised CFS scheduler:
 -  We note that [Global fair clock - Wtotal i (k)] = run time Rtotal i (k) !
 -  Minimizing over the quantities [Global fair clock - Wtotal i (k)] is equivalent to minimizing over the accumulated run times Rtotal i (k) 1 st
 -  CFS scheduler had to track complex values like the global fair clock, and accumulated wait times
 -  These both needed the # runnable tasks N(k) at each scheduling decision time k, which keeps changing
 -  New approach just sums run times given each task
 -  this simple approach still achieves fairness according to our derivation

# Virtual Run Time
 -  Revised CFS scheduler simply sums the run times given each task and chooses the one to schedule with the minimum sum
 -  This is equivalent to choosing the task owed the most time on an ideal fair CPU according to our derivation, and thus achieves fairness
 -  Caveat: when a new task is added to the run queue, it may have been blocked a long time, so its run time may be very low compared to other tasks in the run queue
 -  Such a task would consume a long time before its accumulated run time rises to a level close to the other executing tasks' total run times, which would effectively block other tasks from running in a timely manner
 -  Revised CFS scheduler accommodates new tasks as follows:
 -  Define a virtual run time vruntime
 -  As before, each normally running task i simply adds its given run times to its own accumulated sum vruntime i
 -  When a new task is added to the run queue (or an existing task becomes unblocked from I/O), assign it a new virtual run time = minimum of current vruntimes in the run queue
 -  This quantity is defined as min\_vruntime
 -  This approach re-normalizes the newly active task's run time to about the level of the virtual run times of the currently runnable tasks

# Virtual Run Time
 -  Since each newly active task's is given a re-normalized run time, then the run time calculated is not the actual execution time given a task
 -  Hence we need to define a new term vruntime i (k) , rather than use the absolute accumulated run time Rtotal i (k)
 -  Intuitively, CFS choosing the task with the minimum virtual run time prioritizes the task that been given the least time on the CPU
 -  This is the task that should get service first to ensure fairness

# CFS Scheduler in Linux
 -  So revised CFS scheduler chooses the task with the minimum vruntime i (k) at each scheduling decision time k
 -  This approach is responsive to interactive tasks!
 -  They get instant service after they unblock from their I/O
 -  This is because they are given a re-normalized vruntime i (k) = min\_vruntime ,
 -  Since CFS chooses the next task to schedule as the one with the minimum vruntime, then the interactive task will be chosen first and get service immediately

# CFS' Red Black Tree
 -  To quickly find the task with the minimum vruntime, order the vruntimes in a Red-Black tree
 -  This is a balanced tree, ordered from left (minimum vruntime) to right (maximum vruntime)
 -  As tasks run more, their virtual run time increases
 -  so they migrate to other positions further to the right in the tree
 -  Must re-insert nodes to tree, and rearrange tree, but the RB tree is self-balancing
 -  Tasks that haven't had CPU execution in a while will migrate left and eventually get service
 -  Intuitively, this eventual migration leftwards makes CFS fair

# Chapter 6: Advanced Scheduling
# Completely Fair Scheduler (CFS) in Linux
 -  Linux 2.6.23+/3.\* has a "completely fair" scheduler
 -  Based on concept of an "ideal" multitasking CPU

# CFS Intuition
 -  Example: The current accounting balance is summarized in the table below
 -  Example: let's have round robin over 4 tasks

# CFS Scheduler
 -  Selects the task with the maximum wait time
 -  This is equivalent to selecting the task with the minimum virtual run time
 -  See derivation from last lecture
 -  To quickly find the task with the minimum vruntime, order the vruntimes in a Red-Black tree

# CFS Scheduler and Priorities
 -  All non-RT tasks of differing priorities are combined into one RB tree
 -  Don't need 40 separate run queues, one for each priority - elegant!
 -  Higher priority tasks get larger run time slices
 -  Each task has a weight that is a function of the task's niceness priority
 -  The run time allocated to a task = (a default target latency of 20 ms for desktop systems) \* (task's weight) / (sum of weights of all runnable tasks)
 -  Lower niceness => higher the priority => more run time is given on the CPU

# CFS Scheduler and Priorities
 -  The weight is roughly equivalent to 1024 / (1.25 ^ nice\_value).
 -  So the relative ratio between different niceness priorities is geometric
 -  Recall niceness ranges from -20 to +19
 -  -20 corresponds to Linux priority 100
 -  +19 corresponds to Linux priority 139
 -  Example: tasks 1 & 2 have niceness 0 & 5
 -  Ratio of weights of task 1/task 2  = 1024/335 = 3X
 -  Every 20 ms, Task 1 gets run time = 20 ms\*1024/(1024+335) = 15 ms
 -  Every 20 ms, Task 2 gets run time = 5 ms

# CFS Scheduler and Priorities
 -  In addition, higher priority tasks are scheduled more often
 -  virtual runtime += (actual CPU run time) \*  NICE\_0\_LOAD / task's weight
 -  Higher priority
 -   higher weight
 -   less increment of vruntime
 -   task is further left on the Red-Black tree and is scheduled sooner

# CFS Scheduler and Groups
 -  While CFS is fair to tasks, it is not necessarily fair to applications
 -  Suppose application A1 has 100 threads T1-T100
 -  Suppose application A2 is interactive and has one thread T101
 -  CFS would give A1 100/101 of CPU while giving the interactive app A2 only 1/101 of the CPU
 -  Instead, Linux CFS supports fairness across groups: A1 is in group 1 and A2 is in group 2
 -  Groups 1 and 2 each get 50% of CPU – fair!
 -  Within Group 1, 100 threads share 50% of CPU
 -  Multi-threaded apps don't overwhelm single thread apps

# Real Time Scheduling in Linux
 -  Linux also includes three real-time scheduling classes:
 -  Real time FIFO – soft real time (SCHED\_FIFO)
 -  Real time Round Robin – soft real time (SCHED\_RR)
 -  Real time Earliest Deadline First – hard real time as of Linux 3.14 (SCHED\_DEADLINE)
 -  Only processes with the priorities 0-99 have access to these RT schedulers

# Real Time Scheduling in Linux
 -  "When a Real time FIFO task starts running, it continues to run until it voluntarily yields the processor, blocks or is preempted by a higher-priority real-time task.
 -  It has no timeslices.
 -  All other tasks of lower priority will not be scheduled until it relinquishes the CPU.
 -  Two equal-priority Real time FIFO tasks do not preempt each other."

# Real Time Scheduling in Linux
 -  "SCHED\_RR is similar to SCHED\_FIFO, except that such tasks are allotted timeslices based on their priority and run until they exhaust their timeslice"
 -  Non-real time tasks continue to use CFS algorithm

# Rate-Monotonic Scheduling
 -  For periodic tasks
 -  Each task T i needs the CPU at a period of every  i A task's priority = 1/(period of the task) = 1/  i
 -  Assigns a higher priority to tasks that require the CPU more often, i.e. have smaller task periods
 -  If a task's priority is higher than another task, it preempts that task

# Rate-Monotonic Scheduling
 -  Example 1: tasks T 1 and T 2 have periods  1 = 50 and  2 = 100.
 -  Processing times for T 1 and T 2 are X 1 = 20 and X 2 = 35.
 -  Each task must complete its execution by the start of the next period.
 -  Suppose T 2 is assigned higher priority than T 1 , but there is no preemption (this is not rate monotonic)

# Rate-Monotonic Scheduling
 -  Example 2: same parameters as Example 1, except now assign priorities according to rate-monotonicity  T 1 's priority is higher because T 1 has a shorter periodicity according to rate monotonicity

# Rate-Monotonic Scheduling
 -  Example 3: tasks T 1 and T 2 have periods  1 = 50 and  2 = 80.  Processing times for T 1 and T 2 are X 1 = 25 and X 2 = 35.
 -  T 1 's CPU utilization is 25/50 = 50% while T 2 's CPU utilization is 35/80 = 44%, so system would seem to be schedulable since total utilization = 94% < 100%
 -  T 1 has highest priority by rate monotonicity

# Rate-Monotonic Scheduling
 -  Even if total CPU utilization is less than 100%, the set of tasks may still not schedulable under RM
 -  RM scheduling is considered optimal
 -  if a set of tasks cannot be scheduled by this algorithm, it cannot be scheduled by any other algorithm assigning static priorities

# Multi-core Scheduling
 -  Scheduling over multiple processors or cores is a new challenge.
 -  A single CPU/processor may support multiple cores
 -  In symmetric multi-processing (SMP), each core is self-scheduling.  All modern OSs support some form of SMP.  Two types:
 -  All cores share a single global ready queue.
 -  Another self-scheduling SMP approach is when each core has its own ready queue
 -  Most modern OSs support this paradigm
 -  Caching is important to consider
 -  Each CPU has its own cache to improve performance
 -  If a process migrates too much between CPUs, then have to rebuild L1 and L2 caches each time a process starts on a new core/processor
 -  L3 caches that span multiple cores can help alleviate this, but there is a performance hit, because L3 is slower than L1 and L2.
 -  In any case, L1 and L2 caches still have to be rebuilt.
 -  To maximally exploit caching, processes tend to stick to a given core/processor = processor affinity
 -  In hard affinity, a process specifies via a system call that it insists on staying on a given CPU core
 -  In soft affinity, there is still a bias to stick to a CPU core, but processes can on occasion migrate.
 -  Linux supports both
 -  Load balancing
 -  Goal: Keep workload evenly distributed across cores
 -  Otherwise, some cores will be under-utilized.
 -  When there is a single shared ready queue, there is automatic load balancing
 -  cores just pull in processes from the ready queue whenever they're idle.
 -  Load balancing when there are separate ready Q's,
 -  push migration – a dedicated task periodically checks the load on each core, and if imbalance, pushes processes from more-loaded to less-loaded cores
 -  Pull migration – whenever a core is idle, it tries to pull a process from a neighboring core
 -  Linux and FreeBSD use a combination of pull and push
## Load balancing can conflict with caching
 -  Push/pull migration causes caches to be rebuilt
## Load balancing can conflict with power management
 -  Mobile devices typically want to save power
 -  One approach is to power down unused cores
 -  Load balancing would keep as many cores active as possible, thereby consuming power
## In systems, often conflicting design goals

# Hardware Multithreading
## Multiple hardware threads per core
## Threads can become blocked waiting for data – called a memory stall – so that core waits for a long time
 -  e.g. a cache miss that may take thousands of CPU cycles to retrieve the data from main memory.
 -  Instead of waiting, core switches immediately to another thread to run
 -  Also called hyperthreading (Intel-specific)

# Hardware Multithreading
 -  If N hyperthreads are supported per core, it appears as if there are N logical cores per physical core
 -  Each logical core runs at about 1/N the speed of the physical core
 -  e.g. A dual core dual threaded CPU has four logical cores
 -  Requires 2 levels of scheduling:
 -  map threads to hardware threads;
 -  which hardware thread to run, e.g. RR

# Little's Law
 -  How big of a ready queue do we need?
 -  Employ stochastic queueing theory to answer this question
 -  Consider an abstract ready queue where  = arrival rate of tasks in the queue,  = service rate of tasks exiting the queue
 -  In steady state  must be less than or equal to  , i.e.  -   ≤ 

# Little's Law
 -  How big of a ready queue do we need?
 -  Let W = average wait time in the queue per process
 -  Note W is not the inverse of the service time m.  Analogy: If we have a series of cars that go into a tunnel, eventually they will emerge on the other side, and the steady state service (exit) rate of the cars will equal the arrival (entry) rate.  However, the exit rate doesn't tell us what is the average time spent by each car in the tunnel.  The length of the tunnel (and car speed) tells how long each car spent in the tunnel.

# Little's Law
 -  How big of a ready queue do we need?
 -  Let N = average queue length Then N =  \* W (Little's Law)
 -  In W seconds, on average  \*W tasks arrive in the ready queue.
 -  This conclusion is valid for any scheduling algorithm and arrival distribution.
 -  Useful to know how big on average the queue size N should be for allocation by the OS.

# Chapter 7: Deadlock

# Complete Monitor-based Solution to Dining Philosophers
**monitor** DP {
   status state[5];
    **condition** self[5];

# Deadlock Can Easily Occur
 -  Carefully engineered synchronization solutions to avoid deadlock
 -  The 3 classic synchronization problems like Dining Philosophers, Readers/Writers, and Bounded Buffer P/C saw earlier that semaphores provide mutual exclusion, but can introduce deadlock 2 tasks, each desires a resource locked by the other process
 -  Circular dependency can occur easily due to programming errors, e.g. by switching order of P and V, etc.

# Deadlock Is Hard To Anticipate
 -  Difficult to anticipate by looking at code of a single process or thread
 -  deadlock is a higher-level concept that involves the distributed behavior of multiple processes/threads, e.g. a lengthy circular dependency across many Dining Philosophers
 -  Example:
 -  a Web server talks with a MySQL database which talks with a data mining process which communicates back to the Web server
 -  Each task is written by a different programmer with no advanced knowledge how it might be used with other tasks deadlock is difficult to anticipate, detect, reproduce, prevent, avoid, and recover from

# Deadlock: General Solution?
 -  Want a general solution to deadlock that is not restricted to the solutions for the 3 classic problems of DP, R/W, and BB P/C
 -  A set of processes is in a deadlock state when every process in the set is waiting for an event (e.g. release of a resource) that can only be caused by another process in the set
 -  You have a circular dependency multithreaded and multi-process applications are good candidates for deadlock thread-thread deadlock within a process process-process deadlock

# Modeling Deadlock
 -  Develop a model so we can see circular dependency to use a resource, a process must request() a resource   -- must _wait_ until it's available use() or hold() a resource release() a resource  -  thus, we have resources and processes
 -  Most of the following discussion will focus on reusable resources

# Modeling Deadlock a resource allocation graph can be used to model deadlock try to represent deadlock by a directed graph D(V,E), consisting of vertices V: namely processes and resources and edges E: a request() for a resource R j by a process P i is signified by a directed arrow from process P i → R j a process P i will hold() a resource R j via a directed arrow R j → P i

 -  Example 1:
 -  P1 wants resource R1 but that is held by P2
 -  P2 wants resource R3 but that is held by P3

 -  Example 2:
same graph as before, except now P3 requests instance of R2
Deadlock occurs!
P3 requests R2, which is held by P2, which requests R3, which is held by P3 - this is a loop
P3 → R2→ P2→ R3→ P3
If P1 could somehow release an instance of R2, then we could break the deadlock
But P1 is part of a second loop:
P3 → R2→ P1→ R1→ P2→ R3→ P3
So P1 can't release its instance of R2 if the graph contains cycles or loops, then there _may be the possibility_ of deadlock but does a loop guarantee that there is deadlock?

 -  Example 3:
 -  there is a loop: P1 → R1→ P2→ R2→ P1
 -  In this case, there is no deadlock either P3 can release an instance of R1, or P4 can release an instance of R2 this breaks any possible deadlock cycle if the graph contains cycles or loops, then there may be the possibility of deadlock, but this is not a guarantee of deadlock

# Necessary Conditions for Deadlock
 -  The following 4 conditions must hold simultaneously for deadlock to arise:
 -  Mutual exclusion at least 1 resource is held in a non-sharable mode.  Other requesting processes must wait until the resource is released

# Necessary Conditions for Deadlock
 -  The following 4 conditions must hold simultaneously for deadlock to arise: (continued)
 -  No preemption:
 -  resources cannot be preempted and can only be released voluntarily by the process holding them, after the process is finished.  No OS intervention is allowed.  A process cannot withdraw its request.

# Solutions to Handling Deadlocks
 -  Prevention by OS provide methods to guarantee that at least 1 of the 4 necessary conditions for deadlock does not hold
 -  Avoidance by OS the OS is given advanced information about process requests for various resources this is used to determine whether there is a way for the OS to satisfy the resource requests and avoid deadlock

# Solutions to Handling Deadlocks (2)
 -  Detection and Recovery by OS
 -  Analyze existing system resource allocation, and see if there is a sequence of releases that satisifies every process' needs.
 -  If not, then deadlock is detected, so must recover – drastic action needed, like killing the affected processes!

# Solutions to Handling Deadlocks (3)
 -  Application-level solutions (OS Ignores and Pretends) the most common approach, e.g. UNIX and Windows, based on the assumption that deadlock is relatively infrequent it's up to the application programmer to implement mechanisms that prevent, avoid, detect and deal with application-level deadlock
 -  Map your problem to known deadlock-free solutions: e.g. Bounded Buffer P/C, Readers/Writers problems, Dining Philosophers, …

# Deadlock Prevention: Mutual Exclusion
 -  Prevent the mutual exclusion condition #1 from coming true
 -  This is opposite of our original goal, which was to provide mutual exclusion.
 -  Also, many resources are non-sharable and must be accessed in a mutually exclusive way
 -  example: a printer should print a file X to completion before printing a file Y.  a printer should not print half of file X, and then print the first half of file Y on the same paper thus, it is unrealistic to prevent mutual exclusion

# Deadlock Prevention: Hold and Wait
 -  Prevent the hold and wait condition #2 from coming true prevent a process from holding resources and requesting others
 -  Solution I: request all resources at process creation
 -  Solution II: release all held resources before requesting a set of new ones simultaneously
 -  Solution III: only allow a process to hold one resource at a time

 -  Disadvantages of Hold-and-wait solutions
 -  Solution I: don't know in advance all resources needed
 -  Solutions I & II: poor resource utilization
 -  a process that is holding multiple resources for a long time may only need each resource for a short time during execution
 -  Solution II: possible starvation
 -  a process that needs several popular resources simultaneously may have to wait a very long time

# Deadlock Prevention: Hold and Wait
 -  Solution III: Some processing may require holding more than one resource at a time
 -  e.g. writing a file to a printer may require locking both the file and the printer
 -  Reading a file from a drive may require locking both the file and the drive

# Deadlock Prevention: Hold and Wait
 -  Example: Dining Philosophers Problem prevented hold-and-wait – How?
 -  Enforced a rule that either a philosopher picked up both chopsticks or none at all, i.e. all-or-nothing
 -  Hence no holding one chopstick while waiting on the other chopstick

# Deadlock Prevention: No Preemption
 -  Prevent the "No Preemption" condition #3 from coming true
 -  allow resources to be preempted

 -  Policy I:
 -  If a Process X requests a held resource, then all resources currently held by X are released.
 -  X is restarted only when it can regain all needed resources

# Deadlock Prevention: No Preemption

 -  Policy II:
 -  If a process X requests a resource held by process Y, then preempt the resource from process Y, but only if Y is waiting on another resource
 -  Otherwise, X must wait.  -  the idea is if Y is holding some resources but is waiting on another resource, then Y has no need to keep holding its resources since Y is suspended

# Deadlock Prevention: No Preemption
 -  Disadvantages:
 -  these policies don't apply to all resources, e.g. printers should not be prempted while in the middle of printing, disks should not be preempted while in the middle of writing a block of data
 -  can result in unexpected behavior of processes, since an application developer may not know a priori which policy is being used

# Deadlock Prevention: Circular Wait
 -  Prevent the circular wait condition #4 from coming true
 -  Solution I: a process can only hold 1 resource at a time disadvantage: in some cases, a process needs to hold multiple resources to accomplish a task
 -  Solution II: impose a total ordering of all resource types and require each process to request resources in increasing order this prevents a circular wait - see next slide

# Deadlock Prevention: Circular Wait
 -  Solution II example:
 -  Order all resources into a list: R1, R2, ..., Rm, where R1 < R2 < ... < Rm tape drive = R1, disk drive = R2, printer = R10, temporary buffer = R22
 -  Impose the rule that a process holding R i can only request R j if R j > R i
 -  If a process P holds some R k and requests R j such that R j < R k , then the process must release all such R k , acquire R j , then reacquire R k

# Deadlock Prevention: Circular Wait
 -  Applying ordering of resources to break circular waiting in the Dining Philosophers Problem
 -  R1 < R2 < R3 < R4 < R5
 -  Deadlock happened when all processes first requested their right chopsticks, then requested their left chopsticks
 -  Here, P1 to P4 can all request their right then left chopsticks
 -  But Process P5 requests its left (R1) then right (R5) chopstick due to ordering thus, P5 blocks on R1, not R5, which breaks any possibility of a circular deadlock - why?

# Deadlock Prevention: Circular Wait
 -  Disadvantages of ordering resources:
 -  can lead to poor performance, due to releasing and then reacquiring resources
 -  Difficult to implement in a dynamic resource environment
 -  Coming up with a global scheme for numbering resources

# Deadlock Avoidance
 -  Goal: analyze the system state to see if there is a way to avoid deadlock.
 -  At startup, each process provides OS with information about all of its requests and releases for resources R i
 -  e.g. batch jobs know a priori which resources they'll request, when, and in which order
 -  OS decides whether deadlock will occur at run time

# Deadlock Avoidance
 -  Disadvantage: need a priori info
 -  Simple strategy:
 -  each process specifies a maximum claim knowing all individual future requests and releases is difficult
 -  Having each process estimate its maximum demand for resources is easier and not completely unreasonable
 -  A resource allocation state is defined by # of available resources # of allocated resources to each process maximum demands by each process

# Deadlock Avoidance
 -  A system is in a safe state if there exists a safe sequence of processes <P1, ..., Pn> for the current resource allocation state
 -  A sequence of processes is safe if for each P i in the sequence, the resource requests that P i can still make can be satisfied by: currently available resources + all resources held by all previous processes in the sequence P j , j<i
 -  If resources needed by P i are not available, P i waits for all P j to release their resources

# Deadlock Avoidance
 -  Intuition for a safe state: given that the system is in a certain state, we want to find at least one "way out of trouble"
 -  i.e. find a sequence of processes that, even when they demand their maximum resources, won't deadlock the system
 -  this is a worst-case analysis
 -  it may be that during the normal execution of processes, none ever demands its maximum in a way that causes deadlock
 -  to perform a more optimal (less than worst-case) analysis is more complex, and also requires a record of future accesses

# Deadlock Avoidance
 -  A safe state provides a safe "escape" sequence
 -  A deadlocked state is unsafe
 -  An unsafe state is not necessarily deadlocked
 -  A system may transition from a safe to an unsafe state if a request for resources is granted
 -  ideally, check with each request for resources whether the system is still safe

# Deadlock Avoidance
 -  Example 1:
 -  12 instances of a resource
 -  At time t0, P0 holds 5, P1 holds 2, P2 holds 2
 -  Available = 3 free instances

# Deadlock Avoidance
 -  Example 1 (cont):
 -  Is the system in a safe state?  Can I find a safe sequence?
 -  Yes, I claim the sequence <P1, P0, P2> is safe.
 -  P1 requests its maximum (currently has 2, so needs 2 more) and holds 4, then there is only 1 free resource
 -  Then P1 releases all of its resources, so 5 free
 -  Next, P0 requests its max (currently has 5, so needs 5 more) and holds 10, so that now 0 free
 -  Then P0 releases all its held resources, so 10 free
 -  Next P2 requests its max of 9, leaving 3 free and then releases them all

# Deadlock Avoidance
 -  Example 1 (cont):
 -  Is the system in a safe state?  Can I find a safe sequence?
 -  Yes the sequence <P1, P0, P2> is safe, and is able in the worst-case to request maximum resources for each process in the sequence, and release all such resources for the next process in the sequence
 -  Can this system avoid deadlock?  Yes, we can find a safe sequence.

# Chapter 7: Deadlock Avoidance, Banker's Algorithm

# Intuition: Deadlock Avoidance
 -  Example 2 shows a system moving from a safe to an unsafe state where there could be deadlock
 -  No safe sequence of max requests could be found among all the possibilities
 -  We found a case where there would be deadlock

# Intuition: Deadlock Avoidance
 -  Example 1 shows that even in worst case of max demands there is not deadlock
 -  The system can find an ordered way of granting max requests such that deadlock is avoided
 -  At any given stage, don't grant max requests to tasks whose max requests can't be satisfied
 -  Do grant max requests to remaining tasks
 -  After requesting and getting its max, each such task finishes and releases its resources, adding to the pool of available resources, etc.
#DJIKSTRAS BANKING ALGORITHM

# Chapter 7: Deadlock Detection and Recovery

# Deadlock Detection
 -  When/how often should the detection algorithm run?
 -  Depends on how often deadlock is likely to occur
 -  Depends on how quickly deadlock grows after it occurs, i.e. how many processes get pulled into deadlock and on what time scale
 -  Could check at each resource request – this is costly
 -  Could check periodically – but what is a good time interval?
 -  Could check if CPU utilization suddenly drops
 -  this might be an indication that there's deadlock, and processes are no longer executing, but what's a good threshold?
 -  Could check if resource utilization exceeds some threshold, but what's a good threshold?

# Deadlock Recovery
 -  After OS has detected which processes are deadlocked, then OS can:
 -  Terminate all processes - draconian
 -  Terminate one process at a time until the deadlock cycle is eliminated Check if there is still deadlock after each process is terminated.  If not, then stop.
 -  Preempt some processes – temporarily take away a resource from current owner and give it to another process but don't terminate process e.g. give access to a laser printer – this is risky if you're in middle or printing a document
 -  Rollback some processes to a checkpoint – assuming that processes have saved their state at some checkpoint

# Chapter 8: Memory Management

NOTE!!!!!!
 -  Post-midterm:
 -  Managing memory – Chapters 8-9
 -  Managing files – Chapters 10-12
 -  Other topics: security, networking, VMs, …
NOTE!!!!

# Memory Management
# Memory Hierarchy
 -  cache frequently accessed instructions and/or data in local memory that is faster but also more expensive
 -  L1 = 1 clock cycle (~16 KB)
 -  L2 = 4-5 clock cycles (~1 ns)
 -                (~1 MB)
 -  L3 caches often shared between    cores  ~40 clock cycles
 -      (~10 ns)  (~8-256 MB)
 -  RAM = ~100 cycles/~10-50 ns
 -  Permanent storage:
Flash = 10-100 s
    (depends on read/write,
    flash type, etc.)
Disk = 10 ms
              (107 ns)

# Memory Hierarchy
 -  Most modern CPUs have multiple types of caches:
 -  Instruction cache
 -  Data cache
 -  TLB for caching page table entries
 -  e.g. AMD Athlon K8 has 64 KB L1 instruction cache, 64 KB L1 data cache, and 4 KB L1 TLB, and 1 MB L2 cache

# General Cache Organization (S, E, B)

# Memory Hierarchy
 -  Different types of caches: hardware layout
 -  Fully Associative = one row cached item can go anywhere in cache row
 -  Direct-mapped = one column cached item can only go in a row slot/bin corresponding to its memory address
 -  vs.
 -  N-way Set Associative = N columns and M rows in between, 2,4 usually, there are N items per bin in a cache

# Memory Hierarchy
 -  Different types of caches: write behavior
 -  Write-through on a cache hit: changes to caches written immediately to memory (and in-between caches)
 -  Good for consistency
 -  Bad: may take some time to complete
 -  Write-back on a hit (also called write-behind): lazy writes to memory at some later time, e.g. replacement of a cache line
 -  Good: fast response in writing
 -  Bad: inconsistency between caches and possible loss of cached data

# Memory Hierarchy
 -  Different types of caches: write behavior (cont.)
 -  Write-allocate on a miss:
 -  load into cache, update line in cache
 -  Good if temporal/spatial locality
 -  No-write-allocate on a miss: write immediately to memory
 -  Pairing of write-through with no-write-allocate
 -  Writes go to main memory
 -  Pairing of write-back with write-allocate
 -  Writes go to cache

# Memory Hierarchy
 -  Cache replacement policies:
 -  When the cache gets full, need to find an entry to evict
 -  A common approach is to use
 -  Least Recently Used (LRU)
 -  Evict the data item that has not been read/written for the longest time
 -  Can be implemented by incrementing counters for each cache item on each cache hit and zeroing the most recently fetched/hit item,
 -  least recently used item has the highest counter
 -  more recently used items will have been zeroed more recently and thus have lower counter values – we'll see this more later

# Memory Management
 -  In the previous figure, want newly active process P1 to execute in its own logical address space ranging from 0 to max
 -  It shouldn't have to know exactly where in physical memory its code and data are located
 -  This decouples the compiler from run-time execution
 -  There needs to be a mapping from logical addresses to physical addresses at run time - memory management unit (MMU) takes care of this.
 -  MMU must do:
 -  Address translation: translate logical addresses into physical addresses, i.e. map the logical address space into a
 -  physical address space
 -  Bounds checking: check if the requested memory address is within the upper and lower limits of the address space
 -     One approach is: base register in hardware keeps track of lower limit of the physical address space,  upper limit of physical address space = base register + limit register
 -  base and limit registers provide hardware support for a simple MMU
 -  memory access should not go out of bounds.  If out of bounds, then this is a segmentation fault so trap to the OS.
 -  MMU will detect out-of-bounds memory access and notify OS by throwing an exception
 -  Only the OS can load the base and limit registers while in kernel/supervisor mode
 -  these registers would be loaded as part of a context switch
 -  MMU needs to check if physical memory access is out of bounds
 -  Address Binding at Compile Time:
 -  If you know in advance where in physical memory a process will be placed, then compile your code with absolute physical addresses
 -  Example: LOAD MEM\_ADDR\_X, reg1                            STORE MEM\_ADDR\_Y, reg2    MEM\_ADDR\_X and MEM\_ADDR\_Y are hardwired by the compiler as absolute physical addresses
 -  Address Binding at Load Time
 -  Code is first compiled in relocatable format.  Then replace logical addresses in code with physical addresses during loading
 -  Example: LOAD MEM\_ADDR\_X, reg1                           STORE MEM\_ADDR\_Y, reg2
 -     At load time, the loader replaces all occurrences in the code of MEM\_ADDR\_X and MEM\_ADDR\_Y with (base+MEM\_ADDR\_X) and (base+MEM\_ADDR\_Y).
 -  Once the binary has been thus changed, it is not really portable to any other area of memory, hence load time bound processes are not suitable for swapping (see later slides)
 -  Address Binding at Run Time (most modern OS's do this)
 -  Code is first compiled in relocatable format as if executing in its own logical/virtual address space.
 -  As each instruction is executed, i.e. at run time, the MMU relocates the logical address to a physical address using hardware support such as base/relocation registers.
 -  Example: LOAD MEM\_ADDR\_X, reg1         MEM\_ADDR\_X is compiled as a logical address, and implicitly the hardware MMU will translate it to base+MEM\_ADDR\_X when the instruction executes
 -  For run-time address binding, MMU needs to perform run-time mapping of logical/virtual addresses to physical addresses each logical address is relocated or translated by MMU to a physical address that is used to access main memory/RAM thus the application program never sees the actual physical memory - it just presents a logical address to MMU
 -  Let's combine the MMU's two tasks (bounds checking, and memory mapping) into one figure since logical addresses can't be negative, then lower bound check is unnecessary - just check the upper bound by comparing to the limit register
 -  Also, by checking the limit first, no need to do relocation if out of bounds

# Execution/Run Time Binding
 -  Statically linked executable
 -  Logical addresses are translated instruction by instruction into physical addresses at run time,
 -  and the entire executable has all the code it needs at compile time through static linking
 -  Once a function is statically linked, it is embedded in the code and can't be changed except through recompilation
 -  Your code can contain outdated functions that don't have the latest bug fixes, performance optimizations, and/or feature enhancements

# Run Time Binding with Dynamic Linking
 -  With dynamic linking, the executable does not have all the code it needs at compile time
 -  At compile time, include only a
 -  stub
 -  that contains info on how to find the dynamically linked library function
 -  At run time, translate logical to physical addresses instruction by instruction
 -  But when hitting a stub, the OS looks for the dll function, loads it if it's not already loaded, and replaces itself with a reference to the actual function
 -  Dll is written as position-independent and reentrant code, so it can be put anywhere in memory and executed by multiple processes
 -  UNIX dynamically linked libraries use a .so suffix, eg libfoo.so

# Run Time Binding with Dynamic Linking
 -  Advantages of dynamic linking:
 -  Applications have access to the latest code at run-time, e.g. most recent patched dlls,
 -  Smaller size – stubs stay stubs unless activated
 -  Can have only one copy of the code that is shared among all applications
We'll see later how code is shared between address spaces using page tables

# Swapping
 -  Main memory may not be able to store all processes that are in the ready queue
 -  Use disk/secondary storage to store some processes that are temporarily swapped out of memory
 -  Enables other (swapped in) processes to execute in memory
 -  Special area on disk allocated for this is called backing store or swap space .
 -  This is faster to access – don't go through the normal file system.

# Swapping
 -  When OS scheduler selects process P2, dispatcher checks if P2 is in memory.  If not, there is not enough free memory, then swap out some process P k , and swap in P2

# Swapping
 -  If run time binding is used, then a process can be easily swapped back into a different area of memory.
 -  If compile time or load time binding is used, then process swapping will become very complicated and slow - basically undesirable

# Swapping Difficulties
 -  context-switch time of swapping is very slow
 -  Disks take on the order of 10s-100s of ms
 -  When adding the size of the process to transfer, then transfer time can take seconds
 -  Ideally hide this latency by having other processes to run while swap is taking place behind the scenes,
 -   e.g. in RR, swap out the just-run process, and have enough processes in round robin to run before swap-in completes & newly swapped-in process is ready to run
 -  can't always hide this latency if in-memory processes are blocked on I/O
 -  UNIX avoids swapping unless the memory usage exceeds a  threshold

# Swapping Difficulties
 -  swapping of processes that are blocked or waiting on I/O becomes complicated
 -  one rule is to simply avoid swapping processes with pending I/O
 -  fragmentation of main memory becomes a big issue – see next slide
 -  can also get fragmentation of backing store disk
 -  Modern OS's swap portions of processes in conjunction with virtual memory and demand paging

# Allocation
 -  as processes arrive, they're allocated a space in main memory
 -  over time, processes leave, and memory is deallocated
 -  This results in external fragmentation of main memory
 -  There are many small chunks of non-contiguous unallocated memory between allocated processes in memory
 -  OS must find a large enough unallocated chunk in fragmented memory that a process will fit into

# Allocation
 -  Multiple strategies: best fit - find the smallest chunk that is big enough This results in more and more fragmentation
 -  worst fit - find the largest chunk that is big enough this leaves the largest contiguous unallocated chunk for the next process
 -  first fit - find the 1 st chunk that is big enough This tends to fragment memory near the beginning of the list
 -  next fit – view fragments as forming a circular buffer, find the 1 st
 -  chunk that is big enough after the most recently chosen fragment

# Fragmentation
 -  though there is enough total free memory, there may not be one contiguous chunk of free memory large enough to fit a process
 -  free memory is fragmented in small pieces in RAM
 -  Both first-fit & best-fit aggravate external fragmentation, less so with worst-fit
 -  Solution: periodically compact/de-fragment memory only possible if addresses are bound at run-time
 -  Expensive: translate the address spaces of most if not all processes, and CPU is unable to do other meaningful work during this compaction

# Chapter 13:  Device I/O, Interrupts, DMA

# Von Neumann Computer Architecture

# Modern Computer Architecture: Devices and the I/O Bus

# Classes of Exceptions

# Examples of x86 Exceptions
 -  x86 Pentium: Table of 256 different exception types
 -  some assigned by CPU designers (divide by zero, memory access violations, page faults)
 -  some assigned by OS, e.g. interrupts or traps
 -  Pentium CPU contains exception table base register that points to this table, so it can be located anywhere in memory

# Device System Call Interface
 -  Create a simple standard interface to access most devices
 -  Every I/O device driver should support the following:
 -  open close read write set ( ioctl in UNIX) stop
 -  Other functions as needed, e.g. seek for random access devices...
 -  Not every function will be fully realized for certain I/O devices e.g. devices that are exclusively input would not need to be written to

# Device System Call Interface
 -  Block vs character devices
 -  Character devices generate or process data as a linear stream of bytes
 -  e.g. keyboards, mice, audio, modems, printers
 -  Block devices read/write data in discrete blocks
 -  e.g. most storage devices like disks
 -  Sequential vs direct/random access
 -  Some devices like magnetic tapes are best suited for sequentially reading or writing the medium
 -  Other devices like magnetic/solid-state disks support random access, i.e. the read and writes can access any part of disk in any order

# Device System Call Interface
 -  Blocking versus Non-Blocking I/O
 -  blocking system call: process put on wait queue until I/O read or write completes
 -  non-blocking system call: a write or read returns immediately with partial number of bytes transferred (possibly zero), e.g. keyboard, mouse, network sockets
Makes the application more complex, because not all the data may have been read or written – have to add additional code to handle this, like a loop
 -  Synchronous versus asynchronous
 -  asynchronous returns immediately,
 -  but at some later time, the full number of bytes requested is transferred
 -  – subtle difference with non-blocking definition
 -  Many use synchronous and blocking interchangeably, and asynchronous and non-blocking interchangeably
## Two I/O Methods

# Queuing of Device Requests
 -  More than one application may want access to the same device
 -  OS keeps a queue for each device.
 -  Read/write requests for this device are placed in this queue
 -  Queue is often FIFO
 -  For printers, queuing has the special term spooling
 -  Device requests may be reordered in some cases
 -  Reorder requests to satisfy a performance criterion
Speed of access to a mechanical device, e.g. disk, may be improved by grouping reads/writes to the same location on the device
 -  Priority of an application

# Device Drivers
 -  Support the device system call interface functions open, read, write , etc. for that device
 -  Interact directly with the device controllers
 -  Know the details of what commands the device can handle, how to set/get bits in device controller registers, etc.
 -  Are part of the device-dependent component of the device manager
 -  Control flow:
 -  An I/O system call traps to the kernel, invoking the trap handler for I/O (the device manager), which indexes into a table using the arguments provided to run the correct device driver

# Device Controller Interface

# Device Controller States
 -  Therefore, need 2 bits for 3 states:
 -  A BUSY flag and a DONE flag
 -  BUSY=0, DONE=0 => Idle
 -  BUSY=1, DONE=0 => Working
 -  BUSY=0, DONE=1 => Finished
 -  BUSY=1, DONE=1 => Undefined

# Polling I/O: A Write Example

# Polling I/O Read Operation

# Polling I/O – Busy Waiting
 -  Note that the OS is spinning in a loop twice:
 -  Checking for the device to become idle
 -  Checking for the device to finish the I/O request, so the results can be retrieved
 -  If N devices with requests pending, have to poll all N devices
 -  This wastes CPU cycles that could be devoted to executing applications
 -  Instead, want to overlap CPU and I/O Free up the CPU while the I/O device is processing a read/write

# Device Manager I/O Strategies
 -  Beneath the system call API (blocking/non-blocking or synchronous/asynchronous), OS can implement several strategies for I/O with devices direct I/O with polling the OS device manager busy-waits, we've already seen this direct I/O with interrupts
 -  More efficient than busy waiting DMA with interrupts

# Hardware Interrupts
 -  CPU incorporates a hardware interrupt flag
 -  Whenever a device is finished with a read/write, it communicates to the CPU and raises the flag
 -  Frees up CPU to execute other tasks without having to keep polling devices
 -  Upon an interrupt, the CPU interrupts normal execution, and invokes the OS's interrupt handler
 -  Eventually, after the interrupt is handled and the I/O results processed, the OS resumes normal execution

# Standard CPU Execution
# CPU Execution with Interrupts
# Interrupt Handler
 -  First, save the processor state
 -  Save the executing app's program counter (PC) and CPU register data
 -  Next, find the device causing the interrupt
 -  Consult interrupt controller to find the interrupt offset, or poll the devices
 -  Then, jump to the appropriate device handler
 -  Index into the Interrupt Vector using the interrupt offset
 -  An Interrupt Service Routine (ISR) either refers to the interrupt handler, or the device handler
 -  Finally, reenable interrupts (see later slides)

# Examples of Exceptions in x86 Pentium Systems

# Interrupts From Multiple Devices
 -  How does the interrupt flag get raised if there are multiple devices?
 -  Aggregate the individual flags logically using OR if there is at least one device wanting to interrupt the CPU, then raise the aggregate flag

# Interrupts From Multiple Devices

# Maskable Interrupts
 -  Maskable interrupts can be turned off by CPU before execution of critical instruction sequences
 -  For example, don't want the interrupt handler to be interrupted while it has not yet saved the processor state are used by device controllers to talk with CPU
 -  Nonmaskable interrupts/exceptions are reserved for events such as unrecoverable memory errors and cannot be turned off by the CPU
 -  Can have multiple interrupt priority levels high-priority interrupts can preempt execution of a low-priority interrupt

# Examples of Exceptions in x86 Pentium Systems

# Disabling/Enabling Interrupts

# Interrupt-Driven I/O Operation

# Direct Memory Access (DMA)
 -  The CPU can become a bottleneck if there is a lot of I/O copying data back and forth between memory and devices
 -  Example: want to copy a 1 MB file from disk into memory.
 -  The disk is only capable of delivering memory in say 1 KB blocks through the small data register in its device controller
 -  So every time a 1 KB block is ready to be copied, an interrupt is raised, interrupting the CPU a thousand times for a 1 MB file!
 -  This will slow down execution of normal programs and the OS.
 -  Worst case: CPU could be interrupted after the transfer of every byte/character, or every packet from the network card

# Direct Memory Access (DMA)
 -  DMA solution:
 -  Bypass the CPU for large data copies
 -  Only raise an interrupt at the very end of the data transfer, instead of at every intermediate block
 -  Example: for a 1 MB file, only one interrupt is raised at the end of file transfer to memory, rather than 1000
 -  Substantially improves performance of large I/O transfers

# DMA with Interrupts Example

# Direct Memory Access (DMA)
## Since both CPU and the DMA controller have to move data to/from main memory, how do they share main memory?
## Burst mode
 -  While DMA is transferring, CPU is blocked from accessing memory
## Interleaved mode or "cycle stealing"
 -  DMA transfers one word to/from memory, then CPU accesses memory, then DMA, then CPU, etc… - interleaved
## Transparent mode – DMA only transfers when CPU is not using the system bus
 -  Most efficient but difficult to detect
